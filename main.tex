\documentclass[10pt,a4paper]{scrartcl}

\usepackage[english]{babel}

\input{../Headerfiles/Packages}
\input{../Headerfiles/Titles}
\input{../Headerfiles/Commands}
\graphicspath{{Pictures/}}
\parindent 0pt

\title{System Identification}
\author{
GianAndrea MÃ¼ller\\
\and
Stefan Rickli
}

\newtheorem{define}{Definition}

%additional commands
\newcommand{\ejon}{(e^{j\omega_n})}
\newcommand{\ejo}{(e^{j\omega})}
\newcommand{\ejzw}{(e^{j(\zeta-\omega_n)})}
\newcommand{\ejz}{(e^{j\zeta})}

\begin{document}
\begin{multicols*}{4}
\maketitle
\tableofcontents
\end{multicols*}

\begin{multicols*}{2}
\section{System Identification}
\section{Definitions}

\begin{define}
A system is said to be \textbf{time invariant} if the response to a certain input is not depending on absolute time.
\end{define}

\begin{define}
A system is said to be \textbf{linear} if its output response to a linear combination of inputs is the same as the linear combination of the output responses of the individual inputs.
\end{define}

\begin{define}
A system is said to be \textbf{causal} if the output at a certain time depends on the input up to that time only.
\end{define}

\begin{define}
A process is said to be \textbf{stationary} if it does not depend on time.
\end{define}

\section{Frequency Domain Methods}

\subsection{Sampling Operation}

\importname{Sampling with period $T$}{$y(k) = \left.y(t)\right|_{t=kT,k=0,1,2,\ldots}$}

\subsection{Fourier Series of Periodic Signals}

\important{$X(e^{j\omega_m})=\sum\limits_{k=0}^{M-1}x(k)e^{-j\omega_m k}$}

\mportant{$\omega_m=\frac{2\pi m}{M}=\omega_0$}

Non-negative frequencies are $m=0$ to $m=M/s$.

They correspond to: $\omega_m = 0,\frac{2\pi}{M},\frac{4\pi}{M},\ldots,\frac{2\pi(M/2-1)}{M},\pi$.

\begin{TDefinitionTable*}
$M$&number of samples&\\
$\omega_0=\frac{2\pi}{M}$&fundamental frequency ($y(k)$)&$\siu{\radian}$\\
$T$&sampling time&$\siu{\second}$\\
$\tau_p=MT$&period&$\siu{\second}$\\
$\omega_0=\frac{2\pi}{\tau_p}$&fundamental frequency ($y(t)$)&$\siu{\radian\per\second}$\\
\end{TDefinitionTable*}

\mportant{$0,\underbrace{\frac{2\pi}{\tau_p}}_{\substack{\text{Fundamental}\\\text{frequency}}},\underbrace{2\left(\frac{2\pi}{\tau_p}\right),\ldots,\frac{M}{2}\left(\frac{2\pi}{\tau_p}\right)}_{\text{Harmonics}}$}

\begin{define}
The highest frequency $\omega_u=\omega_{M/2}=\frac{\pi}{T}$ is called the \textbf{Nyquist frequency}.
\end{define}

\myspic{0.5}{NyquistFrequency}

\begin{TPMatlab}
%Non negative frequency vector
omega = omega_n(N,'p0');
%Discrete Fourier Transform
fft(u); %Matlab
DFT(u); %Lecture
\end{TPMatlab}

\begin{tiny}
Note that the definition of the fft in the Matlab documentation does not describe the actual implementation perfectly. The index actually runs from 0 to $N-1$ as in the definition made in class.
\end{tiny}

\section{Spectral Estimation}

\myspic{0.5}{IODiagram}

\mportname{Transfer function}{$Y(j\omega)=G(j\omega)U(j\omega)$}

\mportname{Discrete time TF}{$Y(e^{j\omega}=G(e^{j\omega}U\ejo$}

\mportant{$\frac{\omega_u}{2\pi}=\frac{r}{NT}$}

\begin{TDefinitionTable*}
$\omega_u$&input frequency&$\siu{\rad\per\second}$\\
$N$&calculation length&$\siu{\ }$\\
$T$&experiment duration?&$\siu{\second}$\\
$r$&some integer&$\siu{\ }$\\
\end{TDefinitionTable*}

\mportname{Input}{$u(k)=\alpha\cos(\omega_u k),\ k=0,1,\ldots,K-1\ \text{ with } K\geq N$}

\mportname{Output}{$y(k)=\alpha\left|G(e^{j\omega_u})\right|\cos(\omega_u k+\theta(\omega_u))+v(k)+\text{transient}$}

where $\theta(\omega_u)=arg(G(e^{j\omega_u})$

\subsection{Sinusoidal correlation methods}

Correlation functions:

\mportant{$I_c(N)=\frac{1}{N}\sum\limits_{k=0}^{N-1}y(k)\cos(\omega_uk)$}

\mportant{$I_s(N)=\frac{1}{N}\sum\limits_{k=0}^{N-1}y(k)\sin(\omega_u k)$}

To calculate those from the data:

\small
\begin{align*}
I_c(N)=\frac{\alpha}{2}\left|G(e^{j\omega_u})\right|\cos(\theta(\omega_u)+\frac{\alpha}{2}\left|G(e^{j\omega_u)}\right|\frac{1}{N}\sum\limits_{k=0}^{N-1}\cos(2\omega_u k+\theta(\omega_u))+\frac{1}{N}\sum\limits_{k=0}^{N-1}v(k)\cos(\omega_u k)
\end{align*}
\normalsize

If the noise, $v(k)$ is sufficiently uncorrelated then the variance satisfies,

\mportant{$\lim\limits_{N\rightarrow\infty}\text{var}\left\{\frac{1}{N}\sum\limits_{k=0}^{N-1}v(k)\cos(\omega_uk)\right\} =0$}

with a convergence rate of $1/N$.

Thus in the limit $N\rightarrow\infty$,

\begin{align*}
E\left\{I_c(N)\right\}&\rightarrow\frac{\alpha}{2}\left|G(e^{j\omega_u})\right|\cos(\theta(\omega_u))\\
E\left\{I_s(N)\right\}&\rightarrow-\frac{\alpha}{2}\left|G(e^{j\omega_u})\right|\sin(\theta(\omega_u))
\end{align*}

and since $\lim\limits_{N\rightarrow\infty}\ \text{var}\left\{I_c(N)\right\}=0,\quad\lim\limits_{N\rightarrow\infty}\ \text{var}\left\{I_s(N)\right\}=0$

The transfer function can be estimated via:

\important{$\hat{G}_N(e^{j\omega_u})=\frac{I_c(N)-jI_s(N)}{\alpha/2}$}

\begin{itemize}
\item Advantages
\begin{itemize}
\item Energy is concentrated at the frequencies of interest.
\item Amplitude of $u(k)$ can easily be tuned as a function of frequency.
\item Easy to avoid saturation and tune signal/noise (S/N) ratio.
\end{itemize}
\item Disadvantages
\begin{itemize}
\item A large amount of data is required.
\item Significant amount of time required for experiments.
\item Some processes won't allow sinusoidal inputs.
\end{itemize}
\end{itemize}

\section{Frequency Domain Methods}

%\textbf{Signals considered}
%
%\begin{itemize}
%\item Finite energy
%\item Periodic
%\item Random
%\item Finite length
%\end{itemize}
%
%\textbf{Signal properties of interest}
%
%\begin{itemize}
%\item Autocorrelation
%\item Crosscorrelation
%\item Frequency domain representation
%\item Spectral density (energy or power)
%\end{itemize}

\mportname{Discrete-time domain signal}{$x(k),\ k=-\infty,\ldots,\infty$}

\importname{Fourier Transform}{$X\ejo=\sum\limits_{k=-\infty}^{\infty}x(k)e^{-j\omega k}$}

\begin{itemize}
\item $X\ejo$ is $2\pi$ periodic.
\item If $\sum\limits_{k=-\infty}^\infty|x(k)|<\infty$ then $X\ejo$ converges.
\end{itemize}

\importname{Inverse Fourier Transform}{$x(k)=\frac{1}{2\pi}\int_{-\pi}^{\pi}X\ejo e^{j\omega k}d\omega$}

where $k=-\infty,\ldots,\infty$

\subsection{Finite Energy Signal}

\subsubsection{Energy Spectral Density (Finite Energy Signal)}

If $x(k)$ is a finite energy signal,

\mportant{$||x(k)||_2^2=\sum\limits_{k=-\infty}^{\infty}|x(k)|^2<\infty$}

\importname{Energy Spectral Density}{$S_x\ejo=|X\ejo|^2$}

\begin{itemize}
\item For finite energy signals the energy spectral density is easily calculated, for signals of infinite length and thus infinite energy however, this is not possible. For that reason the \textbf{power} spectral density is calculated instead!
\end{itemize}

\subsubsection{Autocorrelation (Finite Energy Signal)}

\important{$R_x(\tau)=\sum\limits_{k=-\infty}^{\infty}x(k)x(k-\tau),\quad \tau=-\infty,\ldots,0,\ldots,\infty$}

The spectral density is the Fourier Transform of the autocorrelation:

\mportant{$\sum\limits_{\tau=-\infty}^{\infty}R_x(\tau)e^{-j\omega\tau}=S_x\ejo$}

\begin{TPMatlab}
% autocorrelation for a non-periodic, finite energy signal
Correlation('finen',u); %Lecture
xcorr(u); %Matlab

% crosscorrelation for non-periodic, finite energy signals
Correlation('finen',u,y); %Lecture
xcorr(y,u); %Matlab
\end{TPMatlab}

\subsection{Discrete Periodic Signal}

\mportname{Periodic signal}{$x(k)=x(k+M),\quad \forall\ k\in\{-\infty,\infty\}$}

\mportname{Fundamental frequency}{$\omega_0=\frac{2\pi}{M}$}

\begin{itemize}
\item There are only $M$ unique harmonics of the sinusoid $e^{j\omega_0}$.
\item The non-negative harmonic frequencies are,

\mportant{$e^{jn\omega_0},\ n=0,1,\ldots,M/2$}
\end{itemize}

\begin{TPMatlab}
u_period = sqrt(variance)*randn(N,1)+bias;
u = repmat(u_period,periods,1);
\end{TPMatlab}

\subsubsection{Discrete Fourier Series (Discrete Periodic Signal)}

\important{$X(e^{j\omega_n})=\sum\limits_{k=0}^{N-1}x(k)e^{-j\omega_n k},\ \text{where}\ \omega_n =\frac{2\pi n}{N}=n\omega_0$}

\importname{Inverse Transform}{$x(k)=\frac{1}{N}\sum\limits_{k=0}^{N-1}X(e^{j\omega_n})e^{j\omega_nk}$}

\subsubsection{Autocorrelation (Discrete Periodic Signal)}

\important{$R_x(\tau)=\frac{1}{N}\sum\limits_{k=0}^{N-1}x(k)x(k-\tau)$}

The Fourier transform of $R_x(\tau)$ is now defined as the \textbf{power spectral density}, since it is normalized with the signal length.

\important{$\phi_x(e^{j\omega_n})=\sum\limits_{\tau=0}^{N-1}R_x(\tau)e^{-j\omega_n\tau}=\frac{1}{N}|X(e^{j\omega_n})|^2$}

The energy in a single period is:

\mportant{$\sum\limits_{k=0}^{N-1}|x(k)|^2=\sum\limits_{n=0}^{N-1}\phi_x(e^{j\omega_n})$}

\begin{TPMatlab}
Correlation('periodic',u);
\end{TPMatlab}

\subsubsection{Cross-Correlation (Discrete Periodic Signal)}

\important{$R_{yu}(\tau)=\frac{1}{N}\sum\limits_{k=0}^{N-1}y(k)u(k-\tau)$}

The Fourier transform of $R_{yu}(\tau)$ is now defined as the \textbf{cross-spectral density}.

\important{$\phi_{yu}(e^{j\omega_n})=\sum\limits_{\tau = 0}^{N-1}R_{yu}(\tau)e^{-j\omega_n\tau}) = \frac{1}{N}Y(e^{j\omega_n})U^\ast(e^{j\omega_n})$}

\begin{TPMatlab}
Correlation('periodic',u,y);
\end{TPMatlab}

\subsection{Random Signal}

Normally distributed noise:

\important{$e(k)\in\mathcal{N}(0,\lambda)\Rightarrow\begin{cases}\expe{e(k)}=0\text{ (zero mean)}\\\expe{|e(k)|^2}=\lambda\text{ (variance)}\end{cases}$}

The $e(k)$ are independent and identically distributed (i.i.d.).

\begin{TPMatlab}
standard_deviation = 2;
variance = standard_deviation^2;
bias = 0;
N = 1024;
u = randn(N,1)*standard_deviation + bias;
\end{TPMatlab}

\subsubsection{Autocovariance (Random Signal)}

\begin{align*}
R_x(\tau) &= \expe{x(k)x(k-\tau)}\\
&=\expe{x(k)x^\ast(k-\tau)} \text{ (in the complex case)}\\
&=\expe{x(k)x^\ast(x-\tau)} \text{ (in the multivariable case)}
\end{align*}

General (non-stationary, non-zero mean) case:

\begin{align*}
R_x(s,t)&=\expe{(x(s)-\expe{x})(x(t)-\expe{E})}\\
&= \expe{x(s)x(t)}\text{ (if zero mean)}\\
&= R_x(s-t)\text{ (if stationary)}
\end{align*}

Further properties are

\begin{itemize}
\item $R_x(-\tau)=R_x^\ast(\tau)$
\item $R_x(0)\geq |R_x(\tau)|\ \forall\tau>0$
\end{itemize}

\begin{TPMatlab}
xcorr(u)/N; %Lecture 3.37
\end{TPMatlab}

\subsubsection{Power Spectral Density (Random Signal)}

\important{$\phi_x\ejo:=\sum\limits_{\tau=-\infty}^{\infty}R_x(\tau)e^{-j\omega\tau}$ where $\omega\in[-\pi,\pi)$}

For a zero-mean random signal:

\mportant{$\lim\limits_{N\rightarrow\infty}\frac{1}{N}\sum\limits_{k=0}^{N-1}|x(k)|^2=\var{x(k)}=\frac{1}{2\pi}\int_{-\pi}^{\pi}\phi_x\ejo d\omega$}


Further properties are

\begin{itemize}
\item $\phi_x\ejo\in\mathbb{R}$
\item $\phi_x\ejo\geq 0\ \forall\ \omega$
\item $\phi_x\ejo=\phi_x(e^{-j\omega})$ for all real-valued $x(k)$
\end{itemize}

\begin{TPMatlab}
fft(xcorr(u)/N)) %Lecture 3.37 (?)
\end{TPMatlab}

\subsubsection{Cross-Covariance (Random Signal)}

\important{$R_{yu}(\tau)=\expe{(y(k)-\expe{y(k)})(u(k-\tau)-\expe{u(k)}}$}

For zero mean signals:

\mportant{$R_{yu}(\tau)=\expe{y(k)u(k-\tau)}$}

Joint stationarity is required to make the definition dependent on $\tau$ only.

If $R_{yu}(\tau)=0$ for all $\tau$ then $y(k)$ and $u(k)$ are uncorrelated.

\begin{TPMatlab}
xcorr(u,y)/N; %Lecture 3.37 (?)
\end{TPMatlab}

\subsubsection{Cross Power Spectral Density (Random Signal)}

\important{$\phi_{yu}\ejo=\sum\limits_{\tau=-\infty}^\infty R_{yu}(\tau)e^{-j\omega\tau},\ \omega\in[-\pi,\pi)$}

The inverse is,

\mportant{$R_{yu}(\tau)=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_{yu}\ejo e^{j\omega\tau}d\omega$}

\begin{TPMatlab}
fft(xcorr(y,u)/N); %Lecture 3.37 (?)
\end{TPMatlab}

\subsection{Finite Length Signal}

\subsubsection{Discrete-Fourier Transform (Finite Length Signal)}

\important{$X_N(e^{j\omega_n})=\sum\limits_{k=0}^{N-1}x(k)e^{-j\omega_nk},$ where $\omega_n=\frac{2\pi n}{N}$}

The inverse DFT is

\mportant{$x(k)=\frac{1}{N}\sum\limits_{n=0}^{N-1}X_N(e^{j\omega_n})e^{j\omega_n k},\quad k=0,\ldots, N-1$}

\subsubsection{Periodogram (Finite Length Signal)}

\important{$\frac{1}{N}\left|V_N\ejo\right|^2$}

An asymptotically unbiased estimator of the spectrum is

\mportant{$\lim\limits_{N\rightarrow\infty}\expe{\frac{1}{N}|V_N(e^{j\omega}|^2}=\phi_v(\omega)$}

This assumes that the autocorrelation decays quickly enough:

\mportant{$\lim\limits_{N\rightarrow\infty}\frac{1}{N}\sum\limits_{\tau=-N}^N|\tau R_v(\tau)| =0$}

\section{ETFE}

\myspic{0.5}{IODiagram}

Linear, time-invariant system, $g(l)$:

\mportant{$y(k)=\sum\limits_{l=0}^\infty g(l)u(k-l)+v(k),\quad k=0,1,\ldots$}

Assumptions:

\begin{enumerate}
\item causal system: $g(l) = 0,\forall l<0$
\item noise: $E\{v(k)\} = 0$, zero mean, stationary
\end{enumerate}

Given $\{u(k),y(k)\}$ find an estimate $\hat{G}\ejo$ such that it fits the $G\ejo$.

\importname{Bias}{Bias($\hat{G}))G-E\{\hat{G}\}$}

\importname{Variance}{var($(\hat{G})=E\left\{|\hat{G}-E\{\hat{G}\}|^2\right\}$}

\importname{Mean-square error}{MSE$(\hat{G})=E\left\{|G-\hat{G}|^2\right\}$}

Note that MSE$(\hat{G})=$var$(\hat{G})+$Bias$^2(\hat{G})$.

\subsection{Input-output relationship}

For finite energy signals:

\mportant{$y(k) = \sum\limits_{l=0}^\infty g(l)u(k-l)+v(k)$}

\mportant{$Y\ejo=G\ejo U\ejo+V\ejo$}

which in the idealized case leads to:

\important{$\frac{Y\ejo}{U\ejo}=G\ejo+\frac{V(e^j\omega)}{U\ejo}\approx G\ejo$}

In reality we only have $N$ samples:

\mportant{$\underbrace{Y_N(e^{j\omega_n})}_{\text{length-N DFT}} = \sum\limits_{k=0}^{N-1}y(k) e^{-j\omega_n k}\approx \sum\limits_{k=-\infty}^\infty y(k)e^{-j\omega_n k}=Y(e^{j\omega_n}$}

\mportant{$\underbrace{U_N(e^{j\omega_n})}_{\text{length-N DFT}}=\sum\limits_{k=0}^{N-1}u(k)e^{-j\omega_n k}\approx \sum\limits_{k=-\infty}^\infty u(k)e^{-j\omega_n k}=U(e^{j\omega_n})$}

\importname{ETFE}{$\hat{G}_N(e^{j\omega_n}):=\frac{Y_N(e^{j\omega_n})}{U_N(e^{j\omega_n})}$}

\subsection{Periodic input case}

Period $M$ inputs: $u(k)=u(k+M)$

If $sM = N$ for an integer $s$, the fourier series over N samples is equal to the real fourier series!

\mportant{$U_N(e^{j\omega_n}) = U(e^{j\omega_n})\forall \omega_n=\frac{2\pi n}{N},\ n=0,\ldots,N-1$}

Then

\mportant{$Y_N(e^{j\omega_n})=G(e^{j\omega_n})U_N(e^{j\omega_n})+V_N(e^{j\omega_n})$}

\mportant{$\hat{G}_N(e^{j\omega_n})=G(e^{j\omega_n})+\frac{V_N(e^{j\omega_n})}{U_N(e^{j\omega_n})}$}

%\subsubsection{Error properties}

\textbf{Bias:}

\mportant{$E\{\hat{G}_N(e^{j\omega_n})\}=G(e^{j\omega_n})+E\left\{\frac{V_N(e^{j\omega_n})}{U_N(e^{j\omega_n})}\right\}=G(e^{j\omega_n})$}

when assuming zero mean noise. Thus for periodic inputs with $N$ being an integer number of periods, \textbf{the ETFE is unbiased}.

\textbf{Variance:}

For the unbiased case:

\renewcommand{\expe}[1]{\text{E}\left\{#1\right\}}

\mportant{$E\left\{|\hat{G}_N(e^{j\omega_n})-G(e^{j\omega_n})|^2\right\}=\frac{\phi_v(e^{j\omega_n})+\frac{2}{N}c}{\frac{1}{N}|U_N(e^{j\omega_n})|^2}$}

where $|c|\leq C=\sum\limits_{\tau =1}^\infty |\tau R_v(\tau)|$ is assumed to be finite.

For estimates at different frequencies ($\omega_n\neq\omega_i$):

\mportant{$E\left\{(\hat{G}_N(e^{j\omega_n})-G(e^{j\omega_n}))(\hat{G}_N(e^{-j\omega_i)}-G(e^{-j\omega_i}))\right\}=0$}

\textbf{Transient responses:}

Initial transient corrupts the measurement

\mportant{$y(k)=G(u_{periodic}(k)W_{[0,N-1]}(k))+v(k)$}

with the window function:

\mportant{$W_{[0,N-1]}(k)=\begin{cases}
1&\text{ if } 0\leq0<N\\
0&\text{ otherwise}\end{cases}$}

For all outputs up to time $k=N-1$

\mportant{$y(k)=Gu_{periodic}(k)-\underbrace{G(u_{periodic}W_{(-\infty,-1)})}_{r(k)}+v(k)$}

\mportant{$Y_N(e^{j\omega_n})=G(e^{j\omega_n})U_N(e^{j\omega_n})+R_N(e^{j\omega_n})+V_N(e^{j\omega_n})$}

The input in negative time, which is present in a ideal periodic input, and missing in a real periodic input, has an influence on positive time, which is described by $r(k)$.

\vspace{3ex}

When using a periodic signal multiple times the resulting DFT does not contain more information, since in a periodic signal there are only a certain number of frequencies contained, but the energy in those frequencies increases!

\textbf{Transient bias error:}

\mportant{$\hat{G}\ejon=\frac{Y_N\ejon}{U_N\ejon}=G\ejon+\frac{R_N\ejon}{U_N\ejon}+\frac{V_N\ejon}{U_N\ejon}$}

\textbf{For periodic $u(k)$}

As $N=mM,m\rightarrow\infty$

\mportant{$|U_N\ejon|=m|U_M\ejon|$}

\textbf{For random $u(k)$}

As $N\rightarrow\infty$

\mportant{$E\{|U_N\ejon|\}\rightarrow\sqrt{N}\sqrt{\phi_u\ejon}$}

Thus

\important{$\left|\frac{R_N\ejon}{U_N\ejon}\right|\rightarrow 0$ with rate $\begin{cases}
\frac{1}{N}&\text{ for periodic input}\\
\frac{1}{\sqrt{N}}&\text{ for random inputs}\end{cases}$}

A fix for getting rid of the influence of the transient response: Get rid of the first period.

\begin{TPMatlab}
r = 4;
N_split = N/r;
u_re = reshape(u(N_split+1:end),N_split,[]);
y_re = reshape(y(N_split+1:end),N_split,[]);

U_re = fft(u_re);
Y_re = fft(y_re);

G_re = Y_re./U_re;

G_avg = mean(G_re,2);
\end{TPMatlab}

\subsection{Spectral Transformations}

If $v(k) = 0$

\mportant{$\phi_y\ejon=G\ejon\phi_u\ejon G^T\ejon$}

where $G^T\ejon$ is the complex conjugate of $G\ejon$.

If $v(k)\neq 0$ and uncorrelated

\mportant{$\phi_y\ejon = |G\ejon|^2\phi_u\ejon+|H\ejon|^2$}

But this approach has no more phase information. For that reason use the cross spectrum:

\mportant{$\phi_{yu}\ejon=G\ejon\phi_u\ejon+\phi_{uv}\ejon = G\ejon\phi_u\ejon$}

if $u(k)$ and $v(k)$ are uncorrelated.

\importname{Spectral estimation methods}{$\hat{G}\ejo = \frac{\hat{\phi}_{yu}\ejon}{\hat{\phi}_u\ejon}$}

where

\mportant{$\phi_y\ejon=|G\ejon|^2\phi_u\ejon+\phi_v\ejon$}

\mportant{$\phi_{yu}\ejon=G\ejon\phi_u\ejon$}

\subsection{Approaches to spectral estimation}

\subsubsection{Spectral estimation (Periodogram)}

The periodogram is an asymptotically unbiased estimator of the spectrum given $\lim\limits_{n\rightarrow\infty}\frac{1}{N}\sum\limits_{\tau=-\N}^N|\tau R_v(\tau)|=0$

\mportname{Periodogram}{$\frac{1}{N}|V_N\ejon|^2$}

\important{$\lim\limits_{N\rightarrow\infty}E\left\{\frac{1}{N}|V_N\ejon|^2\right\}=\phi_v\ejon$}

which is under the assumption

\mportant{$\lim\limits_{N\rightarrow\infty}\frac{1}{N}\sum\limits_{\tau=-N}^{N}|\tau R_v(\tau)|=0$}

\subsubsection{Spectral estimation (via covariances)}
\label{sec:SpectralEstimationCovariance}

The autocovariance of the noise for stochastic $v(k)$ is described as:

\mportant{$\hat{R}_v(\tau)=\begin{cases}
\frac{1}{N-|\tau|}\sum\limits_{k=\tau}^{N_1}v(k)v(k-\tau),&\text{for }\tau\geq 0\\
\frac{1}{N-|\tau|}\sum\limits_{k=0}^{N+\tau-1}v(k)v(k-\tau),&\text{for }\tau<0
\end{cases}$}

This is an unbiased estimator of $R_v(\tau)$: $\expe{\hat{R}_v(\tau)}=R_v(\tau)$

\important{$\hat{\phi}_v\ejon=\sum\limits_{\tau=-N+1}^{N-1}\hat{R}_v(\tau)e^{-j\omega \tau}$}

\begin{TPMatlab}
xcorr(a-mean(a))-xcov(a) == 0 %true
\end{TPMatlab}

The functions above both calculate the zero-mean shifted autocorrelation of a signal which is \textbf{not} equivalent to the autocovariance of the signal! They lack the normalization by $N-|\tau|$!

\begin{TPMatlab}
Covariance('zero-mean',a);
Covariance('zero-mean',a,b);
\end{TPMatlab}

\subsubsection{Spectral estimation (periodic signals)}

Periodic signal $x(k)$ with period $M$, $N=mM$ for some integer $m$

\mportant{$R_x(\tau)=\frac{1}{M}\sum\limits_{k=0}^{M-1}x(k)x(k-\tau)$}

The power spectral density can be calculated and is equal to the periodogram:

\important{$\phi_x\ejon=\sum\limits_{\tau = 0}^{M-1}R_x(\tau)e^{-j\omega_n\tau}=\frac{1}{M}\left|X_M\ejon\right|^2$}

\begin{TPMatlab}
Ru = Correlation('periodic',u);
\end{TPMatlab}

\subsubsection{Spectral estimation (more general case)}

Alternative autocorrelation estimate:

\mportant{$\hat{R}_x(\tau)=\begin{cases}
\frac{1}{N}\sum\limits_{k=\tau}^{N-1}x(k)x(k-\tau),&\text{ for }\tau\geq 0\\
\frac{1}{N}\sum\limits_{k=0}^{N+\tau-1}x(k)x(k-\tau),&\text{ for }\tau<0\end{cases}$}

Periodic $x(k)$: unbiased (exact) if $N=mM$

\vspace{3ex}

Random $x(k)$ biased $\expe{\hat{R}_x(\tau)}=\frac{N-|\tau|}{N}R_x(\tau)$.

asymptotically biased as $N\rightarrow\infty,\tau/N\rightarrow 0$

\begin{TPMatlab}
Ru = xcorr(u)/N;
\end{TPMatlab}

\begin{itemize}
\item The bias when estimating $R_x$ for a random signal has the form a triangular weighting across $\tau$. A scaling of the signal would remove the bias, which is effectively done by the spectral estimation for via covariances \ref{sec:SpectralEstimationCovariance}.
\end{itemize}

\section{Averaging and Smoothing}

Multiple experiments $u_r(k),y_r(k), r=1\ldots,R, k=0,\ldots, K-1$

\mportant{$\hat{G}\ejon=\sum\limits_{r=1}^R\alpha_r\hat{G}_r\ejon$}

where $\sum\limits_{r=1}^G\alpha_r = 1$ and for calculating the average $\alpha_r =\frac{1}{R}$.

\vspace{3ex}

The averaging can be optimized by selecting $\alpha_r$ such that the variance $\sigma_r^2\ejon$ is minimized.

\mportant{$\var{\hat{G}\ejon}=\var{\sum\limits_{r=1}^R\alpha_r\ejon \hat{G}_r\ejon}=\sum\limits_{r=1}^R\alpha_r^2\sigma_r^2\ejon$}

This is minimized by

\important{$\alpha_r\ejon=\frac{1/\sigma_r^2\ejon}{\sum\limits_{r=1}^T1/\sigma_r^2\ejon}$}

Thus the signal is weighted inversely proportional to the variance.

\vspace{3ex}

Thus if $\var{\hat{G}_r\ejon}=\frac{\phi_v\ejon}{\frac{1}{N}|U_r\ejon|^2}$ then $\alpha_r\ejon=\frac{|U_r\ejon|^2}{\sum\limits_{r=1}^R|U_r\ejon|^2}$.

\vspace{3ex}

The best result is obtained if the input is the same for all $r$, which will lead to a reduction of the variance as follows:

\mportant{$\var{\hat{G}\ejon}=\frac{\var{\hat{G}_r\ejon}}{R}$}

Biased estimates will reduce the improvement in variance.

\begin{itemize}
\item Since we are adding complex numbers the magnitude of the average is not equal to the average of the magnitudes $r_i$.
\end{itemize}

\subsection{Bias-variance trade-offs in data record splitting}

Divide a data record into smaller parts for averaging:

\mportant{$\{u(k),y(k)\}, k=0,\ldots,K-1$}

Choose $R$ records and calculation length $N$, such that $NR\leq K$:

\mportant{$u_r(n) = u(rN+n)$}

And average the resulting estimates:

\important{$\hat{G}\ejon=\frac{1}{R}\sum\limits_{r=0}^{R-1}\hat{G}_r\ejon=\frac{1}{R}\sum\limits_{r=0}^{R-1}\frac{\hat{Y}_r\ejon}{\hat{U}_r\ejon}$}

As $R$ increases:
\begin{itemize}
\item The number of points calculated, $N$ decreases.
\item The variance decreases (by up to $1/R$).
\item The bias increases (due to non-periodicity transients).
\end{itemize}

Mean-square error
\begin{itemize}
\item Transient bias grows linearly with the number of data splits.
\item Variance decays with a rate of up to $1/$(number of averages).
\end{itemize}

What if there is no option of running periodic input experiments? $\rightarrow$ exploit the assumed smoothness of the underlying system.

\subsection{Smoothing the ETFE}

Assume the true system to be close to constant for a range of frequencies: $G(e^{j\omega_{n+r}})\approx G(e^{j\omega_n})$ for $r=0,\pm 1,\ldots,\pm r$.

The minimum variance smoothed estimate is:

\mportant{$\tilde{G}_N\ejon=\frac{\sum\limits_{r=-R}^R\alpha_r\hat{G}_N(e^{j\omega_{n+r}})}{\sum\limits_{r=-R}^R\alpha_r},\qquad \alpha_r=\frac{\frac{1}{N}|U_N(e^{j\omega_{r+n}})|^2}{\phi_v(e^{j\omega_{n+r}})}$}

The summation above can then be approximated by an integral:

\mportant{$\approx \frac{\int_{\omega_{n-r}}^{\omega_{n+r}}\alpha\ejz\hat{G}_N\ejz d\zeta}{\int_{\omega_{n-r}}^{\omega_{n+r}}\alpha\ejz d\zeta},\qquad \text{with } \alpha\ejz=\frac{\frac{1}{N}|U_N\ejz|^2}{\phi_v\ejz}$}

Which can be reformulated using a smoothing window:

\mportant{$\tilde{G}_N\ejon=\frac{\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\alpha\ejz\hat{G}_N\ejz d\zeta}{\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\alpha\ejz d\zeta}\qquad\text{with }\alpha\ejz=\frac{\frac{1}{N}|U_N\ejz|^2}{\phi_v\ejz}$}

\subsubsection{Assumptions on $\phi_v\ejo$}

Assume $\phi_v\ejo$ is also a smooth function of frequency.

\mportant{$\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\left|\frac{1}{\phi_v\ejz}-\frac{1}{\phi\ejon}\right| d\zeta\approx 0$}

Then use,

\mportant{$\alpha\ejz=\frac{\frac{1}{N}|U_N\ejz|^2}{\phi_v\ejon}$}

to get

\important{$\tilde{G}_N\ejon=\frac{\frac{1}{2\pi}\int_{-pi}^\pi W_\gamma(e^{-j(\zeta-\omega_n)})\frac{1}{N}|U_N\ejz|^2\hat{G}_N\ejz d\zeta}{\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejz^2d\zeta}$}

The wider the frequency window (decreasing $\gamma$)

\begin{itemize}
\item the more adjacent frequencies included in the smoothness estimate.
\item the smoother the result.
\item the lower the noise induced variance.
\item the higher the bias.
\end{itemize}

\subsubsection{Characteristic windows}

\importname{Bartlett}{$W_\gamma\ejo=\frac{1}{\gamma}\left(\frac{\sin\gamma\omega/2}{\sin\omega/2}\right)^2$}

\importname{Hann}{$W_\gamma\ejo=\frac{1}{2}D_\gamma(\omega)+\frac{1}{4}D_\gamma(\omega-\pi/\gamma)+\frac{1}{4}D_\gamma(\omega+\pi/\gamma)$}

where 

\mportant{$D_\gamma(\omega)=\frac{\sin\omega(\gamma+0.5)}{\sin\omega/2}$}

\textbf{Properties of window functions:}

\begin{itemize}
\item $\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejz d\zeta=1$
\item $\int_{-\pi}^\pi\zeta W_\gamma\ejz d\zeta=0$
\item $M(\gamma):=\int_{-\pi}^\pi\zeta^2W_\gamma\ejz d\zeta$
\item $\bar{W}(\gamma):=2\pi\int_{-\pi}^\pi W_\gamma^2\ejz d\zeta$
\end{itemize}

\begin{tabular}{lll}
Bartlett&$M(\gamma)=\frac{2.78}{\gamma},$&$\bar{W}(\gamma)\approx 0.67\gamma\quad$(for $\gamma>5$)\\
Hamming&$M(\gamma)=\frac{\pi^2}{2\gamma^2},$&$\bar{W}(\gamma)\approx 0.75\gamma\quad$(for $\gamma>5$)
\end{tabular}

\begin{itemize}
\item $M(\gamma)$ gives an idea of the bias effect.
\item $\bar{W}(\gamma)$ gives an idea of the variance effect.
\end{itemize}

\subsubsection{Asymptotic bias properties}

\mportant{$\expe{\tilde{G}\ejon-\expe{G\ejon}}=\expe{\tilde{G}\ejon-G\ejon}=M(\gamma)\left(\frac{1}{2}\underbrace{G''\ejon}_\text{curvature}+\underbrace{G'\ejon}_{slope}\frac{\phi_u'\ejon}{\phi_u\ejon}\right)+H.O.T.$}

Increasing $\gamma$

\begin{itemize}
\item makes the frequency window smaller.
\item averages over fewer frequency values.
\item makes $M(\gamma)$ smaller
\item reduces the bias of the smoothed estimate $\tilde{G}\ejon$
\end{itemize}

\subsubsection{Asymptotic variance properties}

\mportant{$\expe{(\tilde{G}\ejon-\expe{\tilde{G}\ejon})^2}=\frac{1}{N}\bar{W}(\gamma)\frac{\phi_v\ejon}{\phi_u\ejon}+H.O.T.$}

Increasing $\gamma$

\begin{itemize}
\item makes the frequency window narrower.
\item averages over fewer frequency values.
\item makes $\bar{W}_\gamma$ larger.
\item increases the variance of the smoothed estimate $\tilde{G}\ejon$.
\end{itemize}

\subsubsection{Asymptotic MSE properties}

\mportant{$\expe{|\tilde{G}\ejon-G\ejon|^2}\approx M^2(\gamma)|F\ejon|^2+\frac{1}{N}\bar{W}(\gamma)\frac{\phi_v\ejon}{\phi_u\ejon}$}

where 

\mportant{$F\ejon=\frac{1}{2}G''\ejon+G'\ejon\frac{\phi'_u\ejon}{\phi_u\ejon}$}

If $M(\gamma)=M/\gamma^2$ and $\bar{W}(\gamma)=\bar{W}\gamma$ then MSE is minised by:

\important{$\gamma_{optimal}=\left(\frac{4M^2|F\ejon|^2\phi_u\ejon}{\bar{W}\phi_v\ejon}\right)^{1/5}N^{1/5}$}

and 

\mportant{MSE at $\gamma_{optimal}\approx CN^{-4/5}$}

\section{Windowing and Input Signals}

\mportant{$\phi_{yu}\ejo=G\ejo\phi_u\ejo$}

\important{$\hat{G}\ejon=\frac{\hat{\phi}_{yu}\ejon}{\hat{\phi}_u\ejon}$}

Recall that the smoothed ETFE is:

\mportant{$\tilde{G}_N\ejon=\frac{\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejz|^2\hat{G}_N\ejz d\zeta}{\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma \ejzw\frac{1}{N}|U_n\ejz|^2d\zeta}$}

The denominator term approaches $\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw \phi\ejon d\zeta$ as $N\rightarrow \infty$. 

\vspace{3ex}

If in addition $W_\gamma\ejo$ is concentrated around $\zeta = 0$ (i.e. $\gamma/N\rightarrow 0$) then the denominator term approaches $\phi_u\ejon$ as $N\rightarrow\infty$.

\vspace{3ex}

This motivates the smoothed spectral estimate:

\important{$\tilde{\phi}_u\ejon=\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejo|^2d\zeta$}

Similarly the numerator approaches $\phi_{yu}$ as $N\rightarrow\infty$:

\important{$\tilde{\phi}_{yu}\ejon=\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejo|^2\hat{G}_N\ejo d\zeta$}

For this reason the smoothed ETFE is equal to the smoothed spectral estimate for $N\rightarrow \infty$.

\subsection{Frequency domain smoothing in matlab}

\begin{TPMatlab}
gamma = 80;
U = fft(u); Y = fft(y);
G_est = Y./U;
G_est_smooth = G_est*0;

[om,Wg] = WfHann(g,N);
%shift to start at zero
zidx = find(om == 0);
omega = [om(zidx:N);om(1:zidx-1)];
Wg = [Wg(zidx:N) Wg(1:zidx-1)];
%variance weighting
a = U.*conj(U);
\end{TPMatlab}

\begin{TPMatlab}
for wn = 1:N
	%reset normalization
	Wnorm = 0;
    for xi = 1:N
    	%wrap window index
        widx = mod(xi-wn,N)+1;
        G_est_smooth(wn) = G_est_smooth(wn) +...
            Wg(widx)*G_est(xi)*a(xi);
        Wnorm = Wnorm + Wg(widx)*a(xi);
    end
    %weigh normalisation
    G_est_smooth(wn) = G_est_smooth(wn)/Wnorm;
end
\end{TPMatlab}

\subsection{Time domain windows}

Define, via the inverse Fourier transform a time domain window:

\mportant{$\omega_\gamma(\tau)=\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejo e^{j\zeta\tau}d\zeta$}

Then the smoothed input spectral estimate $\tilde{\phi}_u\ejon$ is:

\mportant{$\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejo|^2 d\zeta\approx\sum\limits_{\tau-\infty}^\infty \omega_\gamma(\tau)\hat{R}_u(\tau)e^{-j\tau\omega_n}$}

where

\mportant{$\omega_\gamma=\begin{cases}0 &\text{for }\tau<-\gamma\\>0&\text{for }-\gamma\leq\tau\leq\gamma\\0&\text{for }\tau>\gamma\end{cases}$}

where often $\gamma<< N$, which enables the faster calculated redefinition:

\important{$\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\frac{1}{N}|U_N\ejo|^2 d\zeta\approx\sum\limits_{\tau-\gamma}^\gamma \omega_\gamma(\tau)\hat{R}_u(\tau)e^{-j\tau\omega_n}$}

The cross spectral estimate can also be formulated as a convolution in the frequency domain which leads to the analogous formulation to the spectral estimate of $u$:

\important{$\tilde{\phi}_u\ejon=\sum\limits_{\tau=-\gamma}^\gamma \omega_\gamma(\tau)\hat{R}_u(\tau)e^{-j\tau\omega_n}$}

\important{$\tilde{\phi}_{yu}\ejon=\sum\limits_{\tau=-\gamma}^\gamma \omega_\gamma(\tau)\hat{R}_{yu}(\tau)e^{-j\tau\omega_n}$}

\subsubsection{Time domain smoothing in matlab}

\begin{TPMatlab}
gamma = 80;
[~,Wg] = WtHann(gamma,N);
R_u = xcorr(u,N/2)/N; %Lecture 3.37
R_yu = xcorr(y,u,N/2)/N;

omega = Omega_n(N);
phi_u = zeros(size(omega));
phi_yu = zeros(size(omega));

tau = -gamma:gamma;
ind = tau+N/2;
for i = 1:N
%Lecture 5.9 and 5.11
    phi_u(i) = sum(Wg(tau+N/2).*R_u(ind).*exp(-1j.*tau.'.*omega(i)));
    phi_yu(i) = sum(Wg(tau+N/2).*R_yu(ind).*exp(-1j*tau.'.*omega(i)));
end
G_est_smooth = phi_yu./phi_u;
\end{TPMatlab}

\subsubsection{Window characteristics}

Decreasing $\gamma$: narrower $\omega_\gamma(\tau)$, wider $W_\gamma\ejo$
\begin{itemize}
\item the more frequencies, $\hat{G}\ejon$ included in the smoothing.
\item the fewer $\hat{R}(\tau)$ estimates included in the smoothing.
\item the smoother the result.
\item the lower the noise induced variance.
\item the \textbf{higher the bias}.
\end{itemize}

\subsection{Input Signals}

\begin{itemize}
\item Steps
\item Doublet
\item Sinusiods, Chirpts, Multi-Sines
\item Filtered white noise
\item Pseudo-Random Binary Signals (PRBS)
\end{itemize}

\subsubsection{PRBS}

\mportant{$u(k)=a\text{ or }-a$}

\mypic{PRBS}

\mportant{$R_u(\tau)=\frac{1}{N}\sum\limits_{k=0}^{N-1}u(k)u(k-\tau)=\begin{cases}a^2&\text{if }\tau=0\\
\frac{-a^2}{2^X-1}&\text{if }\tau\neq 0\end{cases}$}

\begin{define}
\textbf{Run length} defines how long the signal stays high.
\end{define}

The run length distribution of $u(k)$ is then:

\begin{itemize}
\item[] 1/2 runs of length 1
\item[] 1/4 runs of length 2
\item[] 1/8 runs of length 3
\item[] $\vdots$
\end{itemize}

Other properties:

\begin{itemize}
\item Equal energy at all frequencies.
\item The maximum period of a PRBS signal can be found based on the discrete time dynamic system that generates the signal:

\begin{align*}
A &= \begin{bmatrix}
1&0&0&0&0&1\\
1&0&0&0&0&0\\
0&1&0&0&0&0\\
0&0&1&0&0&0\\
0&0&0&1&0&0\\
0&0&0&0&1&0
\end{bmatrix}
C &= \begin{bmatrix}
0&0&0&0&0&2\gamma
\end{bmatrix}\\
z &= Ax(k)\\
x(k+1) &= \text{mod}_2(z(k))\\
y &= Cx(k)-\gamma
\end{align*}

where $\gamma$ is the amplitude of the signal.

Now the maximum length period result from the idea that $x(k)$ and therefore the output $y(k)$ can only change so many times until it reaches a state it has already held before. From that point on, since the system is first order, the signal repeats. Thus in the longest case, all possible values contained in $x$ are taken exactly once. Thus the maximum periodic length is equal to the number of possible binary numbers with 6 bits, where 6 is the order of the PRBS signal.

\importname{Maximum Length Period}{$2^6-1$}
\end{itemize}

\subsubsection{PRBS in matlab}

\begin{TPMatlab}
signal_order = 8;
singal_length = 2.^signal_order-1;
u = idinput(signal_length,'PRBS');
\end{TPMatlab}

\begin{itemize}
\item Note that \verb+idinput+ only allows the choice of the total length of the signal and derives the fitting signal order such that the signal is at least of that chosen length. A warning is made if the chosen signal length is not equivalent to the run length.
\end{itemize}

\subsubsection{Multi-sinusoidal signals}

\mportant{$u(k)=\sum\limits_{s=1}^S\sqrt{2\alpha_s}\cos(\omega_skT+\phi_s)$}

where $T$ is the sampling period, $\omega_s=\frac{2\pi}{T_P}$, $\frac{T_P}{T}=N$, $S\leq\frac{N}{2}$.

\vspace{3ex}

\textbf{Choose $N$ to be a power of 2 for efficient FFT calculations.}

\vspace{3ex}

\mportname{Total signal power}{$\sum\limits_{s=1}^S\alpha_s = 1$}

\textbf{Schroeder phasing}

Select the phases $\phi_s$ such that the minimize the peak amplitude:

\mportant{$\phi_s=2\pi\sum\limits_{j=1}^s j\alpha_s$.}

for equal power in each sinusoids:

\mportant{$\alpha_s=1/S$ and $\phi_s=\frac{\pi(s^2+s)}{S}$}

\section{Residual Spectra, Coherency, Aperiodicty, Offsets and Drifts}

\subsection{Residual Spectrum}

\subsubsection{Estimating $\phi_v\ejon$}

\mportant{$v(k)=y(k)-G\ejo u(k)$}

\important{$\tilde{\phi}_v\ejon\approx\frac{1}{N}\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma\ejzw\left|Y_N\ejo-\tilde{G}\ejo U_N\ejo\right|^2d\zeta\approx \tilde{\phi}_y\ejon-\frac{\left|\tilde{\phi}_{yu}\ejon\right|^2}{\tilde{\phi}_u\ejon}$}

How much energy is accounted for by the model? How much by noise?

\mportant{$\phi_v\ejon = \phi_y\ejon\left(1-\frac{|\phi_{yu}(\ejon)|^2}{\phi_y\ejon\phi_u\ejon}\right)$}

\importname{Coherency Spectrum}{$\hat{\kappa}_{yu}\ejon=\sqrt{\frac{|\hat{\phi}_{yu}\ejon|^2}{\hat{\phi}_y\ejon\hat{\phi}_u\ejon}}$}

\begin{itemize}
\item If all of the energy in the output is due to the model for a frequency $\omega_n$ then $\hat{\kappa}_{yu}\ejon=1$.
\item This can be used as a measure of effectiveness of the modelling at a particular frequency.
\item Theoretically, $0\leq\hat{\kappa}_{yu}\ejon\leq 1$. One should aim to keep the coherency spectrum as high as possible. It can be adjusted by adjusting the smoothing.
\end{itemize}

\subsection{Time-domain data windowing}

Putting a time domain window directly on the data.

\mportant{$U_w\ejon=\sum\limits_{k=0}^{N-1}w_{data}(k)u(k)e^{-j k\omega_n}$}

often with $w_{data}(k)=w_\gamma(k-N/2)$ (shifted to middle). Typically $\gamma = N/2$ such that all of the data is used.

\myspic{0.5}{TimeDomainWindowing}

For an estimation of the periodogram \textbf{scaling is necessary!}

\importname{Periodogram}{$\frac{1}{E_{scl}}\frac{1}{N} |U_w\ejon|^2\quad\text{where }E_{scl}=\frac{\sum\limits_{k=0}^{N-1}|w_{data}(k)u(k)|^2}{\sum\limits_{k=0}^{N-1}|u(k)|^2}\approx \frac{1}{N}\sum\limits_{k=0}^{N-1}|w_{data}(k)|^2$}

\begin{itemize}
\item Transients influence the periodogram of a sequence, since by the application of the DFT, the signal is assumed to be periodic. Thus even if a sinusoid is sampled - if the periodic extension does not represent the same sinusoid - the DFT will show a range of frequencies, instead of a single distinct one.
\item Time-domain data windowing can help reducing the influence of transients.
\end{itemize}

\subsubsection{Welch's Method}

\myspic{0.5}{Welch}

\begin{enumerate}
\item Split the data record into $L$ overlapping segments of length $N$.
\item $U_l\ejon=\sum\limits_{k=0}^{N-1}w_{data}(k)u_l(k)e^{j\omega_n k}$
\item $\tilde{\phi}_u\ejon=\frac{1}{NLE_{scl}}\sum\limits_{l=1}^L\left|U_l\ejon\right|^2$
\end{enumerate}

\begin{itemize}
\item[+] Windowing can reduce transient response effects.
\item[+] Noise reduction from averaging and windowing.
\item[+] Variance error can be reduced.

\item[-] Windowing can cause energy leakage to adjacent frequencies.
\item[-] Frequency resolution deteriorates.
\item[-] Bias error can be increased.
\item[-] Noise on $u_l(k)$ and $u_{l+1}$ is not uncorrelated.
\item Do not use \verb+welch()+, since it does not fit the definition here.
\end{itemize}

\subsubsection{Drifts and Offsets}

\begin{itemize}
\item Time domain windowing when an offset is present can lead to the introduction of additional frequencies close to zero, which is undesirable.
\item Time domain windowing when a drift is present can lead to the introduction of additional frequencies close to the peak of the sampled sinusoid, which is undesirable as well.
\item A possible solution is \textbf{preprocessing} the data via the assumption

\mportant{$u_d(k)=u(k)-(alpha k+\beta)$}

where $\alpha k+\beta$ is the best linear fit to $u(k)$.
\end{itemize}

\begin{TPMatlab}
detrend(u);
\end{TPMatlab}

\begin{itemize}
\item[+] \verb+detrend+ can completely remove the effect of drifts and offsets.
\item[-] It must be applied carefully, since if no drift/offset is present or if it is detected wrongly, large errors can be introduced.
\end{itemize}

\section{Frequency Domain Subspace ID}

\begin{align*}
x(k+1)&=Ax(k)+Bu(k)\qquad A\in\mathbb{R}^{n_x\times n_x}\\
y(k)&=Cx(k)+Du(k)\qquad D\in\mathbb{R}^{n_y\times n_u}
\end{align*}

\mportname{Pulse response coefficients}{$g(k)=\begin{cases}0&k=0\\D&k=0\\CA^{k-1}&k>0\end{cases}$}

\mportant{$G\ejon=\sum\limits_{k=0}^\infty g(k)e^{-j\omega k}\quad 0\leq \omega\leq\pi$}

Given 

\mportant{$G(n)=G\ejon+V\ejon,\quad n=0,\ldots,N/2$}

Find 

\mportant{$\hat{G}\ejon=\hat{C}\left(e^{j\omega}I-\hat{A}\right)^{-1}\hat{B}+\hat{D}$}

such that

\mportant{$\lim\limits_{N\rightarrow\infty}||\hat{G}\ejon-G\ejon||_\infty =0$}

\importname{Extended observability}{$\mathcal{O}=\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-1}
\end{bmatrix}\in\mathbb{R}^{n_yq\times n_x}$}

rank($\mathcal{O}$) = $n_x$ for all $n_yq\geq n_x$

\importname{Extended controllability}{$\mathcal{C}=\begin{bmatrix}
B&AB&\cdots&A^{r-1}B
\end{bmatrix}\in\mathbb{R}^{n_x\times n_ur}$}

rank($\mathcal{C}$)=$n_x$ for all $n_u r\geq n_x$

The ETFE matches the true transfer function only of there is no noise. In that case the inverse Fourier transform results in the time-aliased impulse response of the system $h_k$: 

\important{$h_k=\frac{1}{N}\sum\limits_{n=0}^{N-1}G\ejon e^{j2\pi kn/N}=CA^{k-1}\left(\sum\limits_{l=0}^\infty A^{Nl}\right)B=CA^{k-1}\left(I-A^N\right)^{-1}B$}

\begin{itemize}
\item We do not get the exact impulse response $g(k)$ since we do not have all the data until $N\rightarrow\infty$.
\end{itemize}

Now the \textbf{Hankel matrix} is formulated with the ultimate goal to extract estimates of the state space matrices.

\mportant{$H=\begin{bmatrix}
h_1&h_2&h_3&\cdots&h_r\\
h_2&h_3&\ddots&\ddots&h_{r+1}\\
h_3&\ddots&\ddots&\ddots&\\
&\ddots&\ddots&&\vdots\\
h_q&h_{q+1}&&\cdots&h_{q+r-1}
\end{bmatrix}\quad$ choose $q>n_x,r>n_x$ and $q+r-1\leq N-1$}

which leads to:

\important{$H=\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-1}
\end{bmatrix}\left(I-A^N\right)^{-1}\begin{bmatrix}
B&AB&\cdots&A^{r-1}B
\end{bmatrix}$}

Next singular value decomposition can be used to decompose the Hankel matrix, which allows to calculate the \textbf{rank} of the state space, as the dimension of $\Sigma_1$.

\mportant{$H=U\Sigma V^T=\begin{bmatrix}
U_1&U_2
\end{bmatrix}\begin{bmatrix}
\Sigma_1&0\\0&0
\end{bmatrix}\begin{bmatrix}
V_1^T\\V_2^T
\end{bmatrix},\quad \Sigma_1\in\mathbb{R}^{n_x\times n_x}$}

Since $U_2$ is multiplied with $\boldsymbol{0}$ the range($H$) = range($U_1$) = $\mathcal{O}$. Which directly leads to span($\{U_1\}$) = span($\mathcal{O}$), which means that the observability matrix is some linear transform of $U_1$. Or differently said, for some choice of $A$ they are equal. \textcolor{red}{probably not correct!?}

\vspace{3ex}

Next an estimate of $\hat{A}$ is made using the observability matrix, for which the selection matrix $J$ is introduced:

\mportant{$J_1\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-1}
\end{bmatrix}=\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-2}
\end{bmatrix}$ and $J_2\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-1}
\end{bmatrix}=\begin{bmatrix}
CA\\\vdots\\CA^{q-1}
\end{bmatrix}$}

Then $j_1\mathcal{O}A=J_2\mathcal{O}$

So $\hat{A}$ is the least squares solution to $J_1\hat{U}_1\hat{A}=J_2\hat{U}_1$.

\vspace{3ex}

And $\hat{C}$ can be found as: $J_e\mathcal{O}=C\Longrightarrow J_3\hat{U}_1=\hat{C}$.

Next, using the initial formulation:

\mportant{$\hat{G}\ejon=\hat{C}\left(e^{j\omega}I-\hat{A}\right)^{-1}\hat{B}+\hat{D}$}

$\hat{B}$ and $\hat{D}$ can be found as the solution to the minimization:

\important{$\hat{B},\hat{D}=\underset{B,D}{\text{argmin}}\sum\limits_{n=0}^N||G\ejon-D-\hat{C}(e^{j\omega_n}I-\hat{A})^{-1}B||_F^2$}

\subsection{Summarizing the subspace identification algorithm}

Uniform data spacing case:

\mportant{$G(n),\quad \omega_n=\frac{\pi n}{N},\quad n=0,\ldots,N/2$}

\begin{enumerate}
\item Extend data to negative frequencies
\mportant{$G(n)=\bar{G}(N-n),\quad n=N/2+1,\ldots,N-1$}
\item Calculate inverse DFT to obtain the time-aliased pulse response
\mportant{$\hat{h}_k=\frac{1}{N}\sum\limits_{n=0}^{N-1}G(n)e^{j 2\pi kn/N},\quad k=0,\ldots,N-1$}

\item Form a block-Hakel matrix:

\mportant{$\hat{H}=\begin{bmatrix}
\hat{h}_1&\hat{h}_2&\hat{h}_3&\cdots\hat{h}_r\\
\hat{h}_2&\hat{h}_3&\ddots&\ddots&\hat{h}_{r+1}\\
\hat{h}_3&\ddots&\ddots&\ddots&\\
&\ddots&\ddots&&\vdots\\
\hat{h}_q&\hat{h}_{q+1}&&\cdots&\hat{h}_{q+r+1}
\end{bmatrix}\in\mathbb{R}^{n_yq\times \nu_r}$}
\item Calculate a singular value decomposition:

\mportant{$\hat{H}=\hat{U}\hat{\Sigma}\hat{V}^T$}

\item Select a model order, $\hat{n}_x$ and partition the SVD:

\mportant{$\hat{U}\hat{\Sigma}\hat{V}^T=\begin{bmatrix}
\hat{U}_1&\hat{U}_2
\end{bmatrix}\begin{bmatrix}
\hat{\Sigma}_1&0\\0&\hat{\Sigma}_2
\end{bmatrix}\begin{bmatrix}
\hat{V}_1^T\\\hat{V}_2^T
\end{bmatrix},\quad \hat{\Sigma}\in\mathbb{R}^{\hat{n}_x\times\hat{n}_x}$}

\item Estimate $\hat{A}$ via:
\begin{align*}
J_1&=\begin{bmatrix}
I_{n_y(q-1)}&O_{n_y(q-1)\times n_y}
\end{bmatrix}\\
J_2&=\begin{bmatrix}
0_{n_q(q-1)}\times n_y&I_{n_y(q-1)}
\end{bmatrix}
\end{align*}

Solve for $\hat{A}$ via LS: 

\important{$J_1\hat{U}_1\hat{A}=J_2\hat{U}_1$}

\item Estimate $\hat{C}$ via

\begin{align*}
J_3&=\begin{bmatrix}
I_{n_y}&0_{n_y\times n_y(q_1)}
\end{bmatrix}
\end{align*}

\important{$\hat{C}=J_3\hat{U}_1$}

\item Find $\hat{B}$ and $\hat{D}$ via least squares:

\important{$\hat{B},\hat{D}=\underset{B,D}{\text{argmin}}\sum\limits_{n=0}^N||G\ejon-D-\hat{C}(e^{j\omega_n}I-\hat{A})^{-1}B||_F^2$}

\item Form the estimate

\important{$\hat{G}(z)=\hat{D}+\hat{C}(zI-\hat{A})^{-1}\hat{B}$}
\end{enumerate}

\begin{itemize}
\item To get real valued $B$ and $D$:

\mportant{$\begin{bmatrix}
\text{real}\left((\hat{C}(\ejon I-\hat{A})^{-1}\right)&I\\\text{imag}\left((\hat{C}(\ejon I-\hat{A})^{-1}\right)&0
\end{bmatrix}\begin{bmatrix}
B\\D
\end{bmatrix}=\begin{bmatrix}
\text{real}(G(n))\\\text{imag}(G(n))
\end{bmatrix}$}
\end{itemize}

\subsubsection{Properties}

\begin{itemize}
\item Asymptotic convergence

\mportant{$\lim\limits_{N\rightarrow\infty}||\hat{G}\ejon- G\ejon||_\infty = 0,\quad n=0,\ldots,N-1$  w.p. 1}
\item The algorithm is \glqq correct\grqq.
If $V\ejon=0$ then there exists a data length $N_0<\infty$ such that:

\mportant{$||\hat{G}_N\ejo-G\ejo||_\infty=0$ for all $N>N_0$}
\end{itemize}

\begin{itemize}
\item[+] Time- and frequency-domain versions available. $N4SID$, etc
\item[+] Many variants which depend on weighting for noise.
\item[+] gives a state-space model directly.
\item[+] Can be effective in determining system order.
\item[+] Works equally well for MIMO systems.
\item[-] Unusual noise weighting in frequency-domain case.
\item[-] Truncated SVD reconstrutions are not Hankel.
\item[-] $\hat{U}_1$ does not have the \glqq shift\grqq structure.
\item[-] Least-squares noise assumptions are not correct.
\item[-] Can give unstable models for stable systems.
\end{itemize}

\subsection{Nonuniformly spaced frequencies}

Time domain

\begin{align*}
x(k+1)&=Ax(k)+Bu(k)\\
y(k)&=Cx(k)+Du(k)
\end{align*}

Frequency domain

\begin{align*}
e^{j\omega}X(\omega)&=AX(\omega)+BU(\omega)\\
Y(\omega)&=CX(\omega)+BU(\omega)
\end{align*}

For a specific frequency $\omega$ on each channel

\mportant{$U_i(\omega)=e_i\quad i=1,\ldots n_u$}

Resulting system equations

\begin{align*}
e^{j\omega}X_i(\omega)&=AX_i(\omega)+BU_i(\omega)\\
Y_i(\omega)&=CX_i(\omega)+DU_i(\omega)
\end{align*}

Defining

\mportant{$X_c(\omega)=\begin{bmatrix}
X_1(\omega)&\cdots&X_{n_u}(\omega)
\end{bmatrix}$}

and stacking the equations column-wise gives:

\begin{align*}
e^{j\omega}X_c(\omega)&=AX_c(\omega)+B\\
G(\omega)&=CX_c(\omega)+D
\end{align*}

Multiplying by $e^{j\omega}$ and substituting, repeating and stacking row-wise:

\mportant{$\begin{bmatrix}
G\ejo\\e^{j\omega}G\ejo\\\vdots\\e^{j(q-1)\omega}G\ejo
\end{bmatrix}=\begin{bmatrix}
C\\CA\\\vdots\\CA^{q-1}
\end{bmatrix}X_c(\omega)+\Gamma\begin{bmatrix}
I_{n_u}\\e^{j\omega}I_{n_u}\\\vdots\\e^{j(q-1)\omega}I_{n_u}
\end{bmatrix}$}

where

\mportant{$\Gamma=\begin{bmatrix}
D&&&\\
CB&D&0&0\\
CAB&\ddots&\ddots&&\\
\vdots&&\ddots&\\
CA^{q-2}B&\cdots&CB&D
\end{bmatrix}$}

Repeat for all frequencies $\omega_i$:

\newcommand{\Gejon}[1]{G(e^{j\omega_{#1}})}
\begin{align*}
\mathcal{G}&=\frac{1}{\sqrt{N}}\begin{bmatrix}
\Gejon{1}&\cdots&\Gejon{N}\\
e^{j\omega_1}\Gejon{1}&&e^{j\omega_N}\Gejon{N}\\
\vdots&&\vdots\\
e^{j(q-1)\omega_1}\Gejon{1}&\cdots&e^{j(q-1)\omega_N}\Gejon{N}
\end{bmatrix}\\
\mathcal{W}&=\frac{1}{\sqrt{N}}\begin{bmatrix}
I&\cdots&I\\
e^{j\omega_1}&&e^{j\omega_N}I\\
\vdots&&\vdots\\
e^{j(q-1)\omega_1}&\cdots&e^{j(q-1)\omega_N}I
\end{bmatrix}\\
\mathcal{X}_c&=\frac{1}{\sqrt{N}}\begin{bmatrix}
X_c(\omega_1)\cdots&X_c(\omega_N)
\end{bmatrix}
\end{align*}

\important{$g=\mathcal{O}\mathcal{X}_c+\Gamma\mathcal{W}$}

Now as $\mathcal{O}$ and $\Gamma$ are real valued

\mportant{$\underbrace{\begin{bmatrix}
\text{real}(\mathcal{G})&\text{imag}(\mathcal{G})
\end{bmatrix}}_{\mathcal{G}_r}=\mathcal{O}\underbrace{\begin{bmatrix}
\text{real}(\mathcal{X}_c)&\text{imag}(\mathcal{X}_c)
\end{bmatrix}}_{\mathcal{X}_{cr}}+\Gamma\underbrace{\begin{bmatrix}\text{real}(\mathcal{W})&\text{imag}(\mathcal{W})\end{bmatrix}}_{\mathcal{W}_r}$}

If $n_yq<n_ur$ then $\exists\mathcal{W}_r^\perp$ such that $\mathcal{W}_r\mathcal{W}_r^\perp$.

\mportant{$\mathcal{G}_r\mathcal{W}_r^\perp=(\mathcal{O}\mathcal{X}_{rc}+\Gamma\mathcal{W}_r)\mathcal{W}_r^\perp=\mathcal{O}\mathcal{X}_{cr}\mathcal{W}_r^\perp$}

\mportant{$\text{range}\left(\mathcal{G}_r\mathcal{W}_r^\perp\right)=\text{range}(\mathcal{O})$}

\begin{TPMatlab}
%To solve linear least squares problem: Ax = b
x = A\b;
\end{TPMatlab}

\section{Closed-Loop ID}

\myspic{0.5}{ClosedLoop}

\begin{TDefinitionTable*}
$r_1(k),r_2(k)$&Excitation\\
$y(k),u(k),r_2(k),r_1(k)$&Measurements\\
\end{TDefinitionTable*}

\mportant{$r(k)=r_2(k)+C(z)r_1(k)$}

\textbf{Motivation for closed-loop ID}

\begin{itemize}
\item Unstable systems must be operated in closed-loop.
\item Operational constraints may require closed-loop.
\item Closed-loop controller maintains the system close to the operating point.
\item Easier to focus identification on specific operation points.
\item Will emphasize plant dynamics close to the cross-over frequency range.
\item Closed-loop operation can remove a large-scale zero-frequency response.
\end{itemize}

\textbf{Methods overview}

\begin{enumerate}
\item Direct (open-loop) methods:

\mportant{$\hat{G}=\hat{Y}_N/\hat{U}_N$ or $\hat{G}=\hat{\phi}_{yu}/\hat{\phi}_u$}

\item Indirect methods

\mportant{$y=\frac{G(z)}{(1+G(z)C(z))}r$}

\item Input-output methods:

\mportant{$y=\frac{G(z)}{(1+G(z)C(z))}r$ and $u=\frac{1}{(1+G(z)C(z))}r$}

\item Dual-Youla methods.
\end{enumerate}

\subsection{Direct Methods}

\begin{align*}
\hat{\phi}_{yu}\ejon&=\frac{1}{2\pi}\int_{-\pi}^\pi W_\gamma(\omega_n-\zeta)\frac{1}{N}Y_N\ejz\bar{U_N}\ejz d\zeta\\
\hat{\phi}_u\ejon&=\frac{1}{2\pi}\int_{-\pi}^{\pi}W_\gamma(\omega_n-\zeta)\frac{1}{N}|U_N\ejz|^2d\zeta\\
\hat{G}\ejon&=\frac{\hat{\phi}_{yu}\ejon}{\hat{\phi}_u\ejon}
\end{align*}

Closed loop transfer functions:

\myspic{0.5}{ClosedLoop}

\begin{align*}
y &= v + G(r_2+C(r_1-y))\\
y &= v + Gr_2+GCr_1-GCy\\
y(1 + GC) &= v + Gr_2 + GCr_1\quad\text{where } S = \frac{1}{1+GC}\\
y &= Sv + \underbrace{SGr_2 + SGCr_1}_{SGr}\quad\text{where } r = r_2 + Cr_1\\
\end{align*}

\sbss{
\begin{align*}
u &= r_2 + C(r_1-(Gu+v))\\
u &= r_2 + Cr_1- CGu-Cv\\
u+CG&= r_2+Cr_1-Cv\\
u &= \underbrace{Sr_2 + SCr_1}_{Sr} - SCv
\end{align*}
}{
\begin{align*}
T_{yr}&=SG\\
T_{ur} &= S
\end{align*}
}

\begin{TPMatlab}
Gdz = c2d(Gs,T_s,'zoh');
Cdz = ...;
Sdz = 1/(1+Gdz*Cdz);
Tdz_y_r1 = 1-Sdz;
Tdz_y_r = feedback(Gdz,Cdz);
Tdz_y_r = Sdz*Gdz;
T_u_r = Sdz;
%Use minreal() to reduce the TF
\end{TPMatlab}

Further these identities can be shown if $r_2,v = 0$:

\begin{TPMatlab}
y = lsim(Tdz_y_r1,r_1,t);
y = lsim(Tdz_y_r,lsim(Cdz,r_1,t),t);
u = lsim(Sdz*Cdz,r_1,t);
u = lsim(Sdz,lsim(Cdz,r_1,t),t);
\end{TPMatlab}

Assume that $\phi_{rv}=0$

\begin{equation*}
\hat{G}=\frac{\hat{\phi}_{yu}}{\hat{\phi}_u}\approx \frac{|S|^2G\hat{\phi}_r-|S|^2\bar{C}\hat{\phi}_v}{|S|^2\hat{\phi}_r+|S|^2|C|^2\hat{\phi}_v}\approx\frac{G\hat{\phi}_r-\bar{C}\hat{\phi}_v}{\hat{\phi}_r+|C|^2\hat{\phi}_v}
\end{equation*}

\begin{itemize}
\item For system identification the used controller $C$ should be not the optimal one for the use case, since a good controller suppresses information \textcolor{red}{Find better explanation}.
\item Reducing the excitation to zero will only deliver information on $C$.
\end{itemize}

\subsection{Input-Output Methods}

\textbf{Identification problems:}

If $\phi_{vr}=0$:

\mportant{$\hat{T}_{yr}\ejon=\frac{Y_N\ejon}{R_N\ejon}$ (asymptotically unbiased)}

\mportant{$\hat{T}_{ur}\ejon=\frac{U_n\ejon}{R_N\ejon}$ (asymptotically unbiased)}

\textbf{Closed-loop identification approach:}

\important{$\frac{T_{yr}}{T_{ur}}=\frac{SG}{S}=G\longrightarrow \hat{G}\ejon=\frac{\hat{T}_{yr}\ejon}{\hat{T}_{ur}\ejon}$}

\begin{itemize}
\item The estimates $\hat{T}_{yr}$ and $\hat{T}_{ur}$ may be unbiased. \textbf{Their ratio is not.}
\item The estimated spectra are weighted by $S\ejon$ or $S\ejon C\ejon$.
\item The noise enters in a complicated manner.
\end{itemize}

\subsubsection{Ratio distributions}

Two normal substitutions $v\in\mathcal{N}(0,1)$ and $w\in\mathcal{N}(0,1)$.

The ratio $z=v/w$ is a stochastic variable with PDF:

\importname{Cauchy Distribution}{$f_z(z)=\frac{1}{\pi}\frac{1}{1+z^2}$}

\begin{itemize}
\item The variance of a Cauchy distribution is infinite.
\end{itemize}

\subsubsection{Averaging closed-loop estimates}

\mportant{$U_l\ejon,Y_l\ejon$}

\begin{align*}
\tilde{G}\ejon&=\frac{1}{L}\sum\limits_{l=1}^L\left(\frac{Y_l\ejon}{U_l\ejon}\right)\\
&\text{or}\\
\tilde{G}\ejon&=\frac{\tilde{Y}\ejon}{\tilde{U}\ejon}
\end{align*}

\begin{itemize}
\item Either top or below is correct which is frequency dependent. \textcolor{red}{Find better explanation}.
\end{itemize}

\subsection{Dual-Youla Methods}

\subsubsection{Youla Parametrizations}

\textbf{Coprime factorizations}

\mportant{$G_0(s)=\frac{N_o(s)}{D_0(s)}$}

with $N_0(s),D_0(s)$ stable and coprime (no common zeros). \textbf{Coprime factorizations are not unique}.

\vspace{3ex}

\textbf{Bezout identity}

The transfer functions $N_0(s)$ and $D_0(s)$ are coprime iff there exists $U(s)$ and $V(s)$ such that

\mportant{$U(s)N_0(s)+V(s)D_0(s) = I$}

\textbf{Normalized Coprime Factorizations}

A coprime factorization is normalized if

\mportant{$D_0^\ast(s)D_0(s)+N_0^\ast(s)N_0(s)=I$}

\begin{TPMatlab}
sncfbal()
\end{TPMatlab}

\textbf{All stablizing controllers}

If we have a controller $C_0$ which stablizes $G_0$ with

\mportant{$C_0=\frac{X_0}{Y_0}$ ($X_0,Y_0$ a coprime factorization)}

then, all controllers, $C$, stablizing $G_0=N_0/D_0$ have the form:

\important{$C_Q=\frac{X_0+QD_0}{Y_0-QN_0}$, with $Q$ stable}

\subsubsection{Dual-Youla methods}

\textbf{Control design:}

Given $G(s)$ select $C(s)$ form the set of controllers stabilizing $G(s)$.

\textbf{Closed-loop identification:}

Given a particular controller $C(s)$ select $G(s)$ from the set of all plants stabilized by $C(s)$.

\textbf{Both problems can be formulated as a search over stable $Q(s)$.}

\vspace{3ex}

\begin{enumerate}
\item \textbf{Formulation}

\mportant{$y=Gu+He\rightarrow Dy=Nu+Fe$}

where $e\in\mathcal{N}(0,1)$ and $D,N,F$ stable. Now find $D,N$ and possibly $F$ from the data, knowing $C_0=X_0/Y_0$.

\item \textbf{Parametrization with $R$ and $F$}

\mportant{$G_R=\frac{N}{D}=\frac{N_0+RY_0}{D_0-RX_0}$ $R$ is stable}

\mportant{$H_{R,F}=\frac{F}{D}=\frac{F}{D_0-RX_0}$ $F$ is stable and stably invertible}
\item \textbf{Equivalent open-loop ID experiment:}

\mportant{$(D_0-RX_0)y=(N_0+RY_0)u+Fe$}

and rearranging

\mportant{$\underbrace{D_0y-N_0u}_{=:\beta}=R\underbrace{(X_0y+Y_0u)}_{=:\alpha}+Fe$}

Open-loop system: $\beta = R\alpha + Fe$ with $R$ and $F$ stable.
\end{enumerate}

\myspic{0.7}{DualYoula}

\begin{itemize}
\item The transfer function from $\beta$ to $\alpha$ is zero. Thus the system is actually open loop! No feedback! 
\end{itemize}

\myspic{0.7}{YoulaOpenLoop}

\begin{align*}
\beta&=D_0y-N_0u \text{ (filtered input and output signals)}\\
\alpha&=X_0y+Y_0u\\
&=X_0y+Y_0\left(r_2+\frac{X_0}{Y_0}(r_1-y)\right)=X_0y+Y_0\left(r-\frac{X_0}{Y_0}y\right)\\
&=Y_0r\text{ (filtered excitation signal)}
\end{align*}

\subsubsection{Summary}

\begin{enumerate}
\item Factorise: $C_0=X_0/Y_0$.
\item Choose excitation: $r$ (Note $\alpha = Y_0r$ filtering).
\item Run closed-loop experiments with $C_0$, measuring $y$ and $u$.
\item Choose an initial model, $P_0=N_0/D_0$ (must be stabilised by $C_0$).
\item Filter measurements, $\beta =D_0y-N_0u$ (time or frequency domain).
\item Filter excitation, $\alpha = Y_0r$.
\item Esimate $\hat{R}$ (and $\hat{F}$) from $\beta = R\alpha + Fe$.
\item Calculate plan estimate, $\hat{G}=(N_0+\hat{R}Y_0)/(D_0-\hat{R}X_0)$.
\end{enumerate}

\section{Time-Domain Correlation Methods}

\subsection{Parametrised Model Sets}

We are looking for the plant, $G$, in a parametrised set

\mportant{$\{G(\theta)\}$} 

where $\theta \in\mathbb{R}^d$ is the parameter vector.

\begin{center}
\begin{tabular}{ll}
Model structure&Parameter vector $\theta\in\mathbb{R}^d$\\
Pulse response: $g(k)$&$\begin{bmatrix}g(0)&g(1)&\ldots\end{bmatrix}$\\
Transfer function: $\frac{B(z)}{A(z)}$&$\begin{bmatrix} a_1&\ldots&b_1&\ldots\end{bmatrix}$\\
State-space: $\left[\begin{tabular} {c|c} A & B\\ \hline  C & D\end{tabular}\right]$&$\begin{bmatrix}A_{ij}&\ldots&B_{ij}&\ldots&C_{ij}\ldots&D_{ij}&\ldots\end{bmatrix}$
\end{tabular}
\end{center}

\subsection{Identification Framework}

\mportname{Measurement data}{$Z_K=\{u(0),y(0),\ldots,u(K-1),y(K-1)\}$}

\mportname{Objective}{$J(\theta,Z_K)$}

\mportname{General optimization formulation}{$\hat{\theta}=\text{arg min}J(\theta,Z_K)$}

\subsubsection{Possible Objectives}

\textbf{Residual error objectives:}

\mportname{error}{$e(k,\theta) = y(k)-G(\theta)u(k)$}

\mportant{$J(\theta) = ||e(\theta)||^2_2$ or $||e(\theta)||_\infty$ or $||e(\theta)||_1$}

\textbf{Parametric error objective:}

\mportant{$J(\theta)=||\theta-\theta_0||_2$ or $E\{\theta-\theta_0\}$}

\textbf{Prediction error objective:}

\mportant{$J(\theta)=E\{y(k+1)-\hat{y}(k+1,\theta|k)\}$}

\subsection{Correlation-Based Methods}

\textbf{Input-Output relationship:}

\mportant{$y(k)=\sum\limits_{i=0}^\infty g(i)u(k-i)+v(k)$}

\important{$R_{yu}(\tau)=g(k)\ast R_u(\tau)$}

This can be written in matrix form (Toeplitz):

\mportant{$\begin{bmatrix}
R_{yu}(0)\\R_{yu}(1)\\R_{yu}(2)\\\vdots
\end{bmatrix}=\begin{bmatrix}
R_u(0)&R_u(-1)&R_u(-2)&\cdots\\
R_u(1)&R_u(0)&R_u(-1)&\\
R_u(2)&R_u(1)&\ddots&\ddots\\
\vdots&&\ddots&\ddots
\end{bmatrix}\begin{bmatrix}
g(0)\\g(1)\\g(2)\\\vdots
\end{bmatrix}$}

Having only a finite data estimate:

\mportant{$\underbrace{\begin{bmatrix}
\hat{R}_{yu}(0)\\\vdots\\\hat{R}_{yu}(N-1)
\end{bmatrix}}_{\hat{R}_{yu}}=\underbrace{\begin{bmatrix}
\hat{R}_u(0)&\cdots&\hat{R}_u(-(N-1))\\
\vdots&&\vdots\\
\hat{R}_{yu}(N-1)&\cdots&\hat{R}_u(0)
\end{bmatrix}}_{\bar{R}_N}\begin{bmatrix}
\hat{g}(0)\\\vdots\\\hat{g}(N-1)
\end{bmatrix}$}

\begin{itemize}
\item Note that $R_u(-\tau)=R_u(\tau)$.
\item In the periodic (noise-free) and FIR case this is exact.
\item $\hat{g}$ is uniquely determined, if $\bar{R}_N$ is invertible, which is given if $u(k)$ is persistently exciting.
\end{itemize}

\subsection{Persistency of Excitation}

A stationary input $u(k)$ is persistently exciting of order $n$ if

\mportant{$\bar{R}_n=\begin{bmatrix}
R_u(0)&\cdots&R_u(-(n-1))\\\vdots&&\vdots\\ R_u(n-1)&\cdots&R_u(0)
\end{bmatrix}$}

is positive definite.

\begin{itemize}
\item This is sufficient to uniquely determine the first $n$ coefficients of the pulse response, $\hat{g}k)$ via the correlation approach.
\item The definition also applies to deterministic signals.
\item A signal is called \textbf{persistently exciting} if this holds for all $n$
\end{itemize}

\textbf{Spectra of persistently exciting signals}

$u(k)$ is persistently exciting of order $n$ if $\phi_u\ejo\neq 0$ for at least $n$ frequencies.

\textbf{Moving average (MA) filtering}

\mportname{$n^{th}$ order MA filter}{$M_n(z)=m_1z^{-1}+\cdots + m_nz^{-n}$}

If for all $n^{th}$ order MA filters,

\mportant{$|M_n\ejo|^2\phi(u)\ejo = 0\Rightarrow M\ejo = 0$}

then $u(k)$ is persistently exciting of order at least $n$.

\begin{itemize}
\item A step function is persistently exciting of order $1$.
\item A $PRBS$ signal is persistently exciting of order $M$.
\item A sum of sinusoids $u(k)=\sum\limits_{s=1}^S\alpha_s\cos(\omega_s k+\phi_s)$ is persistently exciting of order.
\end{itemize}

\mportant{$\begin{cases} 2S&\text{ if } 0<\omega_s<\pi,\ s=1,\ldots,S\\2S-1&\text{ if }\omega = 0\text{ or }\omega=\pi\in\{\omega_s,s=1,\ldots,S\}\\
2S-2&\text{ if }\omega = 0\text{ and } \omega =\pi\in\{\omega_s,s=1,\ldots,S\}\end{cases}$}

\subsection{Autoregressive moving average models}

Model form:

\mportant{$G(z)=\frac{b_1z^{-1}+\ldots+b_mz^{-m}}{1+a_1z^{-1}+\ldots+a_nz^{-n}}$}

Input-Output relationship:

\begin{align*}
y(k)&=G(z)u(k)\\
&=-a_1y(k-1)-\cdots-a_ny(k-n)+b_1u(k-1)+\cdots+b_mu(k-m)\\
&=\phi^T(k)\theta
\end{align*}

where 

\begin{align*}
\phi(k)&=\begin{bmatrix}
-y(k-1)&\cdots&-y(k-n)&u(k-1)&\cdots&u(k-m)
\end{bmatrix}^T\\
\theta&=\begin{bmatrix}
a_1&\cdots&a_n&b_1&\cdots&b_m
\end{bmatrix}^T
\end{align*}

where $\phi(k)$ is called the \textbf{regressor vector} and $\theta$ is called the \textbf{parameter vector}.

\subsubsection{ARX or ARMAX}

Consider $y(k)=\phi^T(k)\theta$ for $k=0,\ldots, N-1$

\mportant{$\underbrace{\begin{bmatrix}
y(0)\\\cdots\\y(N-1)
\end{bmatrix}}_Y=\underbrace{\begin{bmatrix}
\phi^T(0)\\\cdots\\\phi^T(N-1)
\end{bmatrix}}_\Phi\theta$}

or in matrix form: $Y = \Phi\theta$

\textbf{Least squares solution:}

\mportant{$\hat{\theta}=\left(\Phi^T\Phi\right)^{-1}\Phi^TY$}

\begin{TPMatlab}
Theta = Phi\Y;
\end{TPMatlab}

The least squares solution $\hat{\theta}$ solves the problem:

\begin{align*}
\underset{\theta}{minimise} ||\epsilon||_2\\
\text{subject to} Y=\Phi\theta +\epsilon
\end{align*}

Thus the cost function is defined as

\mportant{$J(\theta,Z_N)=||Y-\Phi\theta||_2=||\epsilon||_2$}

\subsubsection{Statistical Properties of the LS Estimate}

\mportname{Model}{$Y=\Phi\theta+\epsilon,\quad\epsilon=\begin{bmatrix}
\epsilon(0)\\\vdots\\\epsilon(N-1)
\end{bmatrix}$}

Error assumptions: $E\{e\} = 0$ and $E\{\epsilon\epsilon^T\} = \sigma^2 I$.

\important{$E\{\hat{\theta}\}=\theta$ (unbiased estimator)}

\important{cov$\{\theta\} = E\{(\hat{\theta}-\theta)(\hat{\theta}-\theta)^T\}=\sigma^2(\Phi^T\Phi)^{-1}$}

\textbf{For a model with correlated noise:}

\mportant{$E\{\epsilon\epsilon^T\} = R$}

\important{$E\{\hat{\theta}\}=\theta$ (unbiased estimator)}

\important{cov$\{\theta\} = E\{(\hat{\theta}-\theta)(\hat{\theta}-\theta)^T\}=(\Phi^T\Phi)^{-1}\Phi^TR\Phi(\Phi^T\Phi)^{-1}$}

Thus the variance of the error depends on the observations $\Phi$!

\subsection{Best linear unbiased estimator (BLUE or Markov estimator)}

\textbf{For a model with correlated noise:}

\mportant{$E\{\epsilon\epsilon^T\} = R$}

Best linear estimator:

\mportant{$\hat{\theta} = Z^TY$ where $Z=R^{-1}\Phi(\Phi^TR^{-1}\Phi)^{-1}$}

satisfies:

\important{$E\{\hat{\theta}\}=\theta$ (unbiased estimator)}


\important{cov$\{\hat{\theta}_Z\} = (\Phi^TR{-1}\Phi)^{-1}\leq$cov$\{\hat{\theta}\}$ for any unbiased estimate}

\begin{itemize}
\item BLUE requires knowledge of the error covariance, $R$, in order to reduce the variance of the error of the estimation.
\end{itemize}

\subsection{Pitfall of Noisy Case}

\myspic{0.7}{ExperimentalSetupNoiseFree}

The setup above is only valid for the noise-free case.
As soon as there is noise the equation derived from the setup above delivers biased results.

\vspace{3ex}

In actuality the problem can only be solved as a linear least squares problem if the setup can be described like this:

\myspic{0.7}{ExperimentalSetupNoisy}

If this is not the case we get biased results. A possible remedy would be:

\begin{enumerate}
\item Assume the system to be of the form:

\begin{equation*}
\begin{bmatrix}
y(0)\\\vdots\\y(N-1)
\end{bmatrix}=\begin{bmatrix}
\phi^T(0)\\
\vdots\\
\phi^T(N-1)
\end{bmatrix}\theta+\begin{bmatrix}
v(0)\\
\vdots
v(N-1)
\end{bmatrix}
\end{equation*}

where $\phi^T(k)=\begin{bmatrix}
-y(k-1)&\cdots&-y(k-N)&u(k-1)&\cdots&u(k-m)
\end{bmatrix}1\begin{bmatrix}
-v(k-1)&\cdots -v(k-N)&0&\cdot&0
\end{bmatrix}$

\item Assume zero noise influence in the regressor $\phi(k)$.
\item Solve with linear least squares.
\item Calculate the $\hat{V} = Y-\Phi\hat{\theta}$ as the residuals.
\item Recalculate taking into account the estimate of the noise $\hat{v}(k)$.
\item Repeat.
\end{enumerate}

\section{Prediction Error Methods}


Given $Z_K=\{u(0),y(0),\ldots,u(K-1),y(K-1)\}$ what is the best estimate of $y(K)$?

Typical assumptions
\begin{itemize}
\item $G(z)$ and $H(z)$ stable.
\item $H(z)$ is stably invertible (no zeros outside the unit disk).


Given $v(k),\ k=0,\ldots,K-1$ can we determine $e(k)$?

\mportname{Inverse filter}{$H_{inv}(z):\quad e8k)=\sum\limits_{i=0}^\infty h_{inv}(i)v(k-i)$}

Note that the inverse filter has to be causal: $h_{inv}(k)=0,k<0$ and stable: $\sum\limits_{k=0}^\infty|h_{inv}(k)|<\infty$.

If $H(z)$ has no zeros for $|z|\geq 1$ then $H_{inv}(z)=\frac{1}{H(z)}$.

\item $e(k)$ has known statistics.
\end{itemize}

\subsection{One step ahead prediction}

Assume $H(z)$ is known, $H(z)$ monic ($h(0) = 1$).

\begin{align*}
v(k)&=\sum\limits_{i=0}^\infty h(i)e(k-i)\\
&= \underbrace{e}_{h(0)v(k)}(k)+\underbrace{\sum\limits_{i=1}^{\infty}h(i)e(k-i)}_{=m(k-1)\text{ \glqq observed\grqq}}
\end{align*}

The above simplification of $h(0)v(k) = e(k)$ only applies if $H(z)$ is monic, i.e. $h(0) = 1$.

\mportname{Prediction based on measurement}{$\hat{v}(k|k-1)$}

which leads to

\mportant{$\hat{v}(k|k-1)=m(k-1)=\sum\limits_{i=1}^{\infty}h(i)e(k-i)$}

The error in this prediction is then $e(k)$ which cannot be reduced.

\vspace{3ex}

In general:

\mportant{Prob$\{x\leq e(k)\leq x+\delta x\}=\int_{x}^{x+\delta x}f_e(x)dx\approx f_e(x)\delta x$}

\mportant{Prob$\{x\leq v(k)\leq x+\delta x|v_{-\infty}^{k-1}\}\leq f_e(x-m(k-1))\delta x$}

which is equivalent to a shift of the distribution with $m(k-1)$. The expected value, the mean or the most likely value (which is not the same for an asymmetric distribution) of the shifted distribution would be a valid choice for $\hat{v}(k|k-1)$.

\important{$\hat{v}(k|k-1)=m(k-1)=\sum\limits_{i=1}^{\infty}h(i)e(k-i)=-\sum\limits_{i=1}^{\infty}h_{inv}(i)v(k-i)\approx -\sum\limits_{i=1}^{k}h_{inv}(i)v(k-i) $}

\subsubsection{Moving Average Model}

\begin{align*}
v(k)&=e(k)+ce(k-1)\Rightarrow H(z) = 1+cz^{-1}\\
H_{inv}(z) &=\frac{1}{1+cz^{-1}}=\sum\limits_{i=0}^\infty (-c)^iz^{-i}
\end{align*}

where $|c|<1$ for stable invertibility.

\important{$\hat{v}(k|k-1)=cv(k-1)-c^2v(k-2)+c^3v(k-3)+\cdots+-(-c)^kv(0)$}

Written recursively:

\begin{align*}
H(z)\hat{v}(k|k-1)&=(H(z)-1)v(k)\\
\hat{v}(k|k-1)+c\hat{v}(k-1|k-2)&=cv(k-1)\\
\hat{v}(k|k-1)&=c\underbrace{(v(k-1)-\hat{v}(k-1|k-2))}_{\epsilon(k-1)}\\
&=c\epsilon(k-1)
\end{align*}

\subsubsection{Autoregressive Noise Model}

\begin{align*}
v(k)&=\sum\limits_{i=0}^\infty a^i e(k-i)\quad |a|<1 \text{ for stablity}\\
H(z)&=\sum\limits_{i=0}^\infty a^iz^{-i}&=\frac{1}{1-az^{-1}}\\
H_{inv}(z)&=1-az^{-1}
\end{align*}

\important{$\hat{v}(k|k-1)=(1-H_{inv}(z))v(k)=av(k-1)$}

\subsection{Output Prediction}

\begin{align*}
y(k)&=G(z)u(k)+v(k)\\
\hat{y}(k|k-1)=\expe{y(k)|Z_K}&=G(z)u(k)+\hat{v}(k|k-1)\\
&=G(z)u(k)+(1-H_{inv}(z))v(k)\\
&=H_{inv}(z)G(z)u(k)+(1-H_{inv}(z))y(k)
\end{align*}

Note that the last line is obtained by replacing $v(k) = y(k) - G(z)u(k)$.

\vspace{3ex}

The prediction error would then be:

\begin{align*}
y(k)-\hat{y}(k|k-1)&=-H_{inv}(z)G(z)u(k)+H_{inv}(z)y(k)\\
&=H_{inv}(z)(y(k)-G(z)u(k))=H_{inv}(z)v(k)\\
&=e(k)
\end{align*}

The one-step ahead predictor and the prediction error are parametrised by $\theta$. This leads to an optimisation problem:

\begin{align*}
J(\theta,Z_K)&=\frac{1}{K}\sum\limits_{k=0}^{K-1}l(\epsilon_F(k,\theta))\text{ where typically } l(\epsilon_F(k,\theta))=||\epsilon_F(k,\theta)||_2\\
\hat{\theta}&=\underset{\theta}{\text{argmin}}J(\theta,Z_K)
\end{align*}

where $\epsilon_F$ is an optionally filtered error signal.

\subsection{Different Models}

\begin{table}[H]
\begin{center}
\begin{tabular}{lll}
Short name&Property&Example\\
\hline
AR&autoregressive&$A(z)y(t)$\\
X&extra&$B(z)u(t)$\\
MA&moving average&$C(z)e(t)$\\\hline
\end{tabular}
\end{center}
\caption{Model naming terminology}
\end{table}

\subsubsection{Equation Error Model Structure: ARX}

\myspic{0.5}{ARX}

\important{$y(t)+a_1y(t-1)+\cdots+a_{n_a}y(t-n_a)=b_1u(t-1)+\cdots+b_{n_b}u(t-n_b)+e(t)$}

\mportant{$\theta=\begin{bmatrix}
a_1&a_2&\cdots&a_{n_a}&b_1&\cdots&b_{n_b}
\end{bmatrix}^T$}

\begin{itemize}
\item[+] The one step ahead predictor directly defines a linear regression.
\item[-] Lack of adequate freedom in describing the properties of the disturbance term.
\end{itemize}

\begin{align*}
\hat{y}(k|\theta)&=H_{inv}(\theta,z)G(\theta,z)u(k)+(1-H_{inv}(\theta,z))y(k)\\
&=B(z)u(k)+(1-A(z))y(k)\\
&=\theta^T\phi(k)=\phi^T(k)\theta
\end{align*}

\importname{$Y-\phi\theta=\epsilon$}{vector of prediction errors}

\begin{itemize}
\item In this case the smallest error is also the smallest prediction error which is not generally the case.
\end{itemize}

\subsubsection{Equation error structure: ARMAX}

\myspic{0.5}{ARMAX}

The ARMAX model structure expands the flexibility of the ARX by describing the equation error as a moving average of white noise.

\important{$\begin{matrix}y(t)+a_1y(t-1)+\cdots +a_{n_a}y(t-n_a)=b_1u(t-1)+\\ \cdots + b_{n_b}u(t-n_b)+e(t)+c_1e(t-1)+\cdots+c_{n_c}e(t-n_c)\end{matrix}$}

with $C(z) = 1+c_1z^{-1}+\cdots+c_{n_c}z^{-n_c}$. The whole thing can be written as:

\mportant{$A(z)y(t)=B(z)u(t)+C(z)e(t)$}

\begin{align*}
\hat{y}(k|\theta)&=\frac{B(z)}{C(z)}u(k)+\left(1-\frac{A(z)}{C(z)}\right)y(k)\\
C(z)\hat{y}(k|\theta)&=B(z)u(k)+(C(z)-A(z))y(k)\\
\hat{y}(k|\theta)&=B(z)u(k)+(1-A(z))y(k)+(C(z)-1)\underbrace{(y(k)-\hat{y}(k|\theta))}_{\epsilon(k)}\\
&=\begin{bmatrix}
b_1&\cdots&a_1&\cdots&c_1&\cdots
\end{bmatrix}\\
&\quad\begin{bmatrix}
u(k-1)&\cdots&-y(k-1)&\cdots&\epsilon(k-1)&\cdots
\end{bmatrix}\\
&=\phi^T(\theta,k)\theta
\end{align*}

This is not linear in $\theta$, since $\epsilon$ depends on the chosen parametrisation, but is called pseudo-linear.

\textbf{Optimisation:}

\begin{align*}
\underset{\theta,\epsilon}{\text{minimise}}||\epsilon||_2\quad \text{(or more generally, $l(\epsilon)$)}\\
\text{subject to }Y=\Phi(\epsilon)^T\theta+\epsilon\quad\text{\textcolor{red}{(nonlinear equality constraint)}}
\end{align*}

\textbf{Constrained minimisation code for ARMAX}

\begin{align*}
A(z) &= 1+a_1z^{-1}+a_2z^{-2}\\
B(z)&= b_1z^{-1}+b_2z^{-2}\\
C(z)&=1+c_1z^{-1}+c_2z^{-2}\\
\theta&=\begin{bmatrix}
b_1&b_2&a_1&a_2&c_1&c_2
\end{bmatrix}^T
\end{align*}

\textcolor{red}{Fix code, probably not running!}

\begin{TPMatlab}
%Create data part of regressor. Assume plant at rest
PhiTyu(1,:) = [0,0,0,0];
PhiTyu(2,:) = [u(1),0,-y(1),0];
for i = 3:K,
	PhiTyu(i,:) = [u(i-1),u(i-2),-y(i-1),-y(i-2)];
end

[x,fval] = fmincon(@(x)ARMAXobjective(x),x0,...
[],[],[],[],[],[],@(x)ARMAXconstraint(x,y,PhiTyu));
function [f] = ARMAXobjective(x) % x = [theta; e]
f = sqrt(x(7:end)'*x(7:end));
function [c,ceq] = ARMAXconstraint(x,y,PhiTyu)
e = x(7:end);
PhiTe = zeros(K,2);
PhiTe(2,1) = e(1);
for j = 3:K,
PhiTe(j,:) = [e(j-1), e(j-2)];
end
ceq = y - [PhiTyu, PhiTe] * theta - e; c = [];
\end{TPMatlab}

\subsubsection{Equation error structure: ARARMAX}

Instead of modelling the equation error as a moving average in can also be described as an autoregression, leading to a ARARX structure. A further generalization using an ARMA description of the equation error leads to the ARARMAX structure below:

\myspic{0.5}{ARARMAX}

\begin{align*}
G(\theta,z)&=\frac{B(\theta,z)}{A(\theta,z)}\\
H(\theta,z)&=\frac{C(\theta,z)}{A(\theta,z)D(\theta,z)}
\end{align*}

\subsubsection{Output Error Model Structure}

Contrary to the equation error model structure, this approach parametrizes the transfer functions independently.

\myspic{0.5}{OutputErrorModel}

\important{$\begin{matrix}w(t,\theta)+f_1w(t-1,\theta)+\cdots+f_{n_f}w(t-n_f,\theta)\\=b_1u(t-1)+\cdots+b_{n_b}u(t-n_b)\end{matrix}$}

\begin{align*}
G(\theta,z)&=\frac{B(\theta,z)}{F(\theta,z)}\\
H(\theta,z)&=1\\
\hat{y}(k|\theta)&=\frac{B(\theta,z)}{F(\theta,z)}u(k)=\phi(k,\theta)^T\theta\\
\theta = \begin{bmatrix}
b_1&b_2&\cdots&b_{n_b}&f_1&f_2&\cdots&f_{n_f}
\end{bmatrix}
\end{align*}

\subsubsection{Box-Jenkins Model Structure}

The Box-Jenkins Model Structure naturally ensues the Output Error, by further modelling the properties of the output error with an ARMA model:

\important{$y(t)=\frac{B(z)}{F(z)}u(t)+\frac{C(z)}{D(z)}e(t)$}

\myspic{0.5}{BoxJenkins}

\begin{align*}
G(\theta,z)&=\frac{B(\theta,z)}{F(\theta,z)}\\
H(\theta,z)&=\frac{C(\theta,z)}{D(\theta,z)}\\
\hat{y}(k|\theta)&=\frac{D(z)}{C(z)}\frac{B(z)}{F(z)}u(k)+(1-\frac{D(z)}{C(z)}y(k)
\end{align*}

\subsubsection{General Model Structure}

The total of the five transfer functions $A(z), B(z),C(z),D(z),E(z)$ enables a total  

\myspic{0.5}{GeneralModel}

\newcommand{\tz}{(\theta,z)}
\begin{align*}
G(\theta,z)&=\frac{B\tz}{A\tz F\tz}\\
H\tz&=\frac{C\tz}{A\tz D\tz}\\
\hat{y}(k|\theta)&=\frac{D(z)}{C(z)}\frac{B(z)}{F(z)}u(k)+(1-\frac{D(z)A(z)}{C(z)})y(k)
\end{align*}

\subsection{Known noise model (with ARMAX dynamics)}

Assume known noise: $v(k)=L(z)e(k)$

Thus

\mportant{$A(z)y(k)=B(z)u(k)+L(z)e(k)$}

\begin{align*}
y_L(k)=L^{-1}(z)y(k)\\
u_L(k)=L^{-1}(z)u(k)
\end{align*}

\mportant{$A(z)y_L(k)=B(z)u_L(k)+e(k)$}

for which least squares gives consistent estimates.

\subsection{High-order model fitting}

Assume an ARARX structure:

\mportant{$A(z)y(k)=B(z)u(k)+\frac{1}{D(z)}e(k)\quad e(k)\sim\mathcal{N}(0,\lambda)$}

Fitting a high order model (order of $D(z)$ is $n_d$)

\mportant{$A(z)D(z)y(k)=B(z)D(z)u(k)+e(k)$}

Least squares estimate with orders $n+n_d$ and $m+n_d$. This gives a consistent estimate of 

\mportant{$\frac{B(z)D(z)}{A(z)D(z)}=\frac{B(z)}{A(z)}$}

This amounts to making the noise model sufficiently rich to capture additional autoregressive features in the noise. In practice though, the cancellation will not be exact. $\hat{A}(z)$ and $\hat{B}(z)$ will be high order.

\section{Parameter estimation statistics}

\subsection{Model description}

As found in chapter 4, Ljung:

\important{$y(t)=G(z,\theta)u(t)+H(z,\theta)e(t),\quad f_e(x,\theta)$}

where $f_e(x,\theta)$ is the PDF of $e(t)$ and $e(t)$ is assumed to be white noise.

\begin{define}
The \textbf{moments} of a PDF describe generalized concept of expected values:

\mportant{$Ee^{m}=\in x^{m}f_e(x) dx=0$}

where $m$ is a positive integer. For $m=1$, the first moment, is the expected value of a PDF, for $m=2$, the second moment, is the variance of a PDF.
\end{define}

\subsection{Basic setup}

\mportname{Parametrised model}{$G=G(\theta,z),\quad H=H(\theta,z)$}

\mportname{Estimation}{$\hat{\theta}=\underset{\theta}{\text{argmin}}J(\theta,Z_K)$}

Consider $K$ observations. Each is a realisation of a random variable, with joint probability distribution

\mportant{$f(x_1,\ldots,x_K|\theta)$}

For independent variables

\mportant{$f(x_1,\ldots,x_K;\theta)=f_1(x_1;\theta)f_2(x_2;\theta)\cdots f_K(x_K;\theta)=\prod\limits_{i=1}^nf_i(x_i;\theta)$}

Substituting the observation $Z_K$ gives a function of $\theta$

\mportname{Likelihood function}{$\mathcal{L}(\theta)=\left.f(x_1,\ldots,x_k;\theta)\right|_{x_i=z_i,i=1,\ldots,K}$}

\mportname{Maximum likelihood estimator}{$\hat{\theta}_{ML}=\underset{\theta}{\text{argmax}}\mathcal{L}(\theta)$}

The log-likelihood is often mathematically easier to consider

\mportant{$\hat{\theta}_{ML}=\underset{\theta}{\text{argmax}}\ln\mathcal{L}(\theta)$}

It will give the same $\hat{\theta}$ since $\ln$ is monotonic.

\subsection{Bayesian approach}

Consider $\theta$ to be a random variable with pdf: $f_\theta(x)$, where $x_i = z_i$. This is an \textbf{a priori} distribution assumed before the experiment. Thus we have some knowledge about the variable that is to be estimated.

\mportname{Conditional Probability}{$\prob{\theta|z_1,\ldots,z_K} = \frac{\prob{Z_K\cap\theta}}{\prob{Z_K}}$}

\mportname{Since $\theta$ is not independent from $Z_K$}{$\prob{Z_K\cap\theta}= \prob{Z_K|\theta}\prob{\theta}$}

which combined results in

\importname{Bayes' Theorem}{$\prob{\theta|z_1,\ldots,z_K}=\frac{\prob{Z_K|\theta}\prob{\theta}}{\prob{Z_K}}$}

where $\prob{\theta}$ contains the prior knowledge.

So

\important{$\underset{\theta}{\text{argmax}}\ f(\theta|z_1,\ldots,z_K)=\underset{\theta}{\text{argmax}}\ f(Z_K|\theta)f_\theta(\theta)$}

where $f_\theta(\theta)$ weights the maximum likelihood estimator towards the expected/most realistic value.

\subsection{Maximum a posteriori (MAP) estimation}

The MAP is closely related to ML estimation but in addition employs an augmented optimization objective which incorporates a prior distribution that quantifies the additionally available information on the estimated quantity.

Given the data $Z_K$

\mportant{$\hat{\theta}_{MAP}=\underset{\theta}{\text{argmax}}\ f(Z_K|\theta)f_\theta(\theta)$}

The maximum likelihood estimator can be interpreted as:

\begin{align*}
\theta_{ML}&=\left.\underset{\theta}{\text{argmax}}f(x_1,\ldots,x_K|\theta)\right|_{x_i=z_i,i=1,\ldots,K}\\
&=\underset{\theta}{\text{argmax}}f(Z_K|\theta)
\end{align*}

This estimate coincides with the simple maximum likelihood estimate if $\theta$ is assumed to be uniformly distributed.

\subsection{CramÃ©r-Rao bound}

The CramÃ©r-Rao bound expresses a lower bound on the variance of unbiased estimators of a deterministic (fixed, though unknown) parameter.

\mportname{Mean-square error matrix}{$P=\expe{(\hat{\theta}(Z_K)-\theta_0)(\hat{\theta}(Z_K)-\theta_0)^T}$}

Assume $\expe{\hat{\theta}(Z_K)}=\theta_0$ and $Z_K\subset\mathcal{R}^K$. Then $P\geq M^{-1}$ where $M$ is the \textbf{Fischer information Matrix}

\begin{align*}
M&=\left.\expe{(\frac{d}{d\theta}\ln f(Z_K|\theta))(\frac{d}{d\theta}\ln f(Z_K|\theta)^T}\right|_{\theta=\theta_0}\\
&=-\left.\expe{\frac{d^2}{d\theta^2}\ln f(Z_K|\theta)}\right|_{\theta=\theta_0}
\end{align*}

Consider a parametrised family of pdfs $f(x_1,\ldots,x_K|\theta)=\prod\limits_{i=1}^Kf_i(x_i|\theta)$

Then

\mportant{$\lim\limits_{K\rightarrow\infty}\hat{\theta}_{ML}\overset{w.p.\ 1}{\longrightarrow}\theta_0$}

and 

\mportant{$\lim\limits_{K\rightarrow\infty}\sqrt{K}(\hat{\theta}_{ML}(Z_K)-\theta_0)\sim\mathcal{N}(0,M^{-1})$}

\subsection{Prediction error statistics}

The prediction error framework is set up as follows:

\mportant{$\epsilon(k,\theta)=y(k)-\hat{y}(k,\theta)$}

where the error $\epsilon(k,\theta)$ is assumed to be independent and identically distributed variables with pfd $f_e(x;\theta)$.

The joint pdf writes as

\mportant{$f(X_K;\theta)=\prod\limits_{k=1}^Kf_e(\epsilon(k,\theta);\theta)$}

\newcommand{\targmax}{\underset{\theta}{\text{argmax }}}
\begin{align*}
\hat{\theta}_{ML}&=\targmax f(X_K;\theta)|_{X_K=Z_K}\\
&=\targmax \mathcal{L}(\theta)\\
&=\targmax \ln f(Z_K|\theta)\\
&=\targmax\frac{1}{K}\sum\limits_{k=1}^K\ln f_e(\epsilon(k,\theta);\theta)
\end{align*}

The prediction error methods give the same estimate if the prediction error cost function is chosen as

\mportant{$l(\epsilon,\theta)=-\ln f_\epsilon(\epsilon|\theta)$}

Thus

\important{$\hat{\theta}_{PE}=\underset{\theta}{\text{argmin }}\frac{1}{K}\sum\limits_{k=1}^K l(\epsilon(k,\theta);\theta)=\hat{\theta}_{ML}$}

\subsection{Linear regression statistics}

One-step ahead predictor

\mportant{$\hat{y}(k|\theta)=\phi^T(k)\theta+\mu(k)$}

In the ARX case $\mu(k)=e(k)$, in other cases $\mu(k)$ can depend on $Z_K$.

\mportant{$\epsilon(k)=y(k)-\phi^t(k)\theta$}

A typical cost function is:

\mportant{$J(\theta,Z_K)=\frac{1}{K}\sum\limits_{k=0}^{K-1}\frac{\epsilon(k)^2}{2}$}

Least-squares criterion:

\mportant{$\hat{\theta}_{LS}=\underbrace{\left(\frac{1}{K}\sum\limits_{k=0}^{K-1}\phi(k)\phi^T(k)\right)^{-1}}_ {R_K^{-1}\in\mathbb{R}^{d\times d}}\underbrace{\frac{1}{K}\sum\limits_{k=0}^{K-1}\phi(k)y(k)}_{f_K\in\mathcal{R}^d}$}

Asymptotic bias:

\mportant{$\lim\limits_{K\rightarrow\infty}\hat{\theta}_{LS}-\theta_0=\lim\limits_{K\rightarrow\infty}R_K^{-1}\frac{1}{K}\sum\limits_{k=0}^{K-1}\phi^T(k)v(k)=(R^\ast)^{-1}f^\ast$}

\mportant{$R^\ast=\expe{\phi(k)\phi^T(k)},\quad f^\ast=\expe{\phi(k)v(k)}$}

Under the assumption that $\lim\limits_{K\rightarrow\infty}\hat{\theta}_{LS}=\theta_0$ we require $(R^\ast)^{-1}f^\ast=0$

So

\begin{enumerate}
\item $R^\ast$ must be non-singular. Persistency of excitation requirement.
\item $f^\ast=\expe{\phi(k)v(k)}=0$ This happens if either
\begin{enumerate}
\item $v(k)$ is zero mean and independent of $\phi(k)$
\item $u(k)$ is independent of $v(k)$ and $G$ is FIR ($n=0$)
\end{enumerate}
\end{enumerate}

This gives

\mportant{$\lim\limits_{K\rightarrow\infty}\sqrt{K}(\hat{\theta}_{LS}-\theta_0)\sim\mathcal{N}(0,\sigma_0^2(R^\ast)^{-1})$}

\subsection{Correlation methods}

Ideally the sequence of prediction errors is white. Thus the prediction errors are uncorrelated with the data.


\textbf{Approach:}
\begin{enumerate}
\item Select a sequence $\zeta(k)$, derived from the past data $Z_K$
\item Require that the error $\epsilon(k,\theta)$ is uncorrelated with $\zeta(k)$
\mportant{$\frac{1}{K}\sum\limits_{k=0}^{K-1}\zeta(k)\epsilon(k,\theta)=0$ could also use $\alpha(\epsilon)$}
\item The ID problem can be then viewed as finding $\theta$ such that this relationship is satisfied

The values $\zeta(k)$ are known as \textbf{instruments}.

Typically $\zeta(k)\in\mathbb{R}^{d\times n_y}$ where $\theta\in\mathbb{R}^d$ and $y(k)\in\mathcal{R}^{n_y}$.
\end{enumerate}

\textbf{Procedure:}
\begin{enumerate}
\item Choose a linear fitter for the prediction errors

\mportant{$\epsilon_F(k,\theta)=F(z)\epsilon(k,\theta)$}

\item Choose a sequence of correlation vectors $\zeta(k,Z_K,\theta)$ constructed from the data
\item Choose a function $\alpha(\epsilon)$ (default is $\alpha(\epsilon)=\epsilon)$ Then

\mportant{$\hat{\theta}=\theta$ solving $f_K(\theta,Z_K)=\frac{1}{K}\sum\limits_{k=0}^{K-1}\zeta(k,\theta)\alpha(\epsilon(k,\theta))=0$}
\end{enumerate}

\subsubsection{Pseudo-Linear Regressions}

For ARX, ARMAX, etc., model structures we can write the predictor

\mportant{$\hat{y}(k|\theta)=\phi^T(k,\theta)\theta$}

This can be solved via LS methods, but alternatively correlation based solutions can be found:

\important{$\hat{\theta}_{PLR}=\theta$ solving $\frac{1}{K}\sum\limits_{k=0}^{K-1}\phi(k,\theta)\underbrace{(y(k)-\phi^T(k,\theta)\theta)}_{\text{prediction error}}=0$}

The prediction errors are orthogonal to the regressor, $\phi(k,\theta)$.

\subsection{Instrumental Variable Methods}

\mportant{$\hat{\theta}_{IV}=\theta$ solving $\frac{1}{K}\sum\limits_{k=0}^{K-1}\zeta(k,\theta)(y(k)-\phi^T(k,\theta)\theta)=0$}

This is solved by

\mportant{$\hat{\theta}_{IV}=\left(\frac{1}{K}\sum\limits_{k=0}^{K-1}\zeta(k)\phi^T(k)\right)^{-1}\frac{1}{K}\sum\limits_{k=0}^{K-1}\zeta(k)y(k)$}

For consistency we require

\mportname{to be nonsingular}{$\expe{\zeta(k)\phi^T(k)}$}

and 

\mportname{(uncorrelated w.r.t. prediction error)}{$\expe{\zeta(k)v(k)}=0$}

\myspic{0.5}{InstrumentalVariables}

\begin{enumerate}
\item Estimate $\hat{\theta}_{LS}$ via linear regression.
\item Select $Q(z)=\hat{A}_{LS}$ and $P(z)=\hat{B}_{LS}$, since the procedure works well when $P$ and $Q$ are close to $B$ and $A$.
\item Calculate $\hat{\theta}_{IV}$
\end{enumerate}

\begin{itemize}
\item Variance and MSE depend on the choice of instruments.
\item Consistency (asymptotically unbiased) is lost if:
\begin{itemize}
\item Noise and instruments are correlated (for example, in closed-loop generating instruments from $u$)
\item Model order selection is incorrect.
\item Filter dynamics cancel plant dynamics.
\item True system is not in the model set.
\end{itemize}
\item Closed-loop approaches: generate instruments from the excitation $r$.
\end{itemize}

\section{Nomenclature}

\begin{TDefinitionTable*}
$y(k)=Gu(k)$&output signal&$\siu{\ }$\\
$u(k)$&input signal&$\siu{\ }$\\
$G$&plant&$\siu{\ }$\\
$\hat{G}=\frac{y}{u}$&estimated plant&$\siu{\ }$\\
$Y\ejo$&output spectrum&$\siu{\ }$\\
$U\ejo$&input spectrum&$\siu{\ }$\\
ZOH&zero order hold&\\
DAC&digital analog converter&\\
ADC&analog digital converter&\\

\end{TDefinitionTable*}

\end{multicols*}

\section{Overviews}
\subsection{Transfer Functions}

\def\myblockwidth{0.225\paperwidth}
\def\myimagewidth{0.8\linewidth}

\footnotesize
\begin{tabular}{|p{\myblockwidth}|p{\myblockwidth}|p{\myblockwidth}|p{\myblockwidth}|}
\hline
\begin{center}

\underline{\textbf{Nonperiodic Time Domain Signal}}

\textcolor{gray}{
\mportant{$x[n]=\int_0^1\hat{x}(\theta)e^{2\pi i n\theta}d\theta$}
}

\vspace{0.4cm}

\mportant{$x(k)=\frac{1}{2\pi}\int_{-\pi}^\pi X\ejo e^{j\omega k}d\omega$}

\vspace{0.2cm}

\includegraphics[width=\myimagewidth]{Pictures/FiniteEnergyTimeSignal.pdf}
\end{center}
&
\begin{center}

\underline{\textbf{Discrete Time Fourier Transform (DTFT)}}

\textcolor{gray}{\mportant{$\hat{x}(\theta)=\sum\limits_{n=-\infty}^\infty x[n]e^{-2\pi in\theta}$}}

\mportant{$X(\omega)=\sum\limits_{k=-\infty}^\infty x(k)e^{-j\omega k}$}

\includegraphics[width=\myimagewidth]{Pictures/DTFTFiniteEnergySignal.pdf}
\end{center}
&
\begin{center}
\underline{\textbf{Energy Spectral Density}}

\vspace{1.5cm}

\mportant{$S_x(\omega)=|X(\omega)|^2=\sum\limits_{\tau=-\infty}^\infty R_x(\tau)e^{-j\omega \tau}$}

\includegraphics[width=\myimagewidth]{Pictures/EnergySpectralDensityFromNonPeriodicAutocorrelation.pdf}
\end{center}
&
\begin{center}
\underline{\textbf{Nonperiodic Autocorrelation}}

\vspace{1.5cm}

\mportant{$R_x(\tau)=\sum\limits_{k=-\infty}^\infty x(k)x(k-\tau)$}
\includegraphics[width=\myimagewidth]{Pictures/NonPeriodicAutocorrelation.pdf}
\end{center}\\\hline

\begin{center}
\underline{\textbf{Periodic Time Domain Signal}}

\textcolor{gray}{\mportant{$x[n]=\frac{1}{N}\sum\limits_{k=0}^{N-1}\hat{x}[k]e^{2\pi in\frac{k}{N}}$}}

\mportant{$x(k)=\frac{1}{N}\sum\limits_{n=0}^{N-1}X(\omega_n)e^{j\omega_n k},\ \omega_n = 2\pi\frac{n}{N}$}

\includegraphics[width=\myimagewidth]{Pictures/PeriodicSignal.pdf}
\end{center}
&
\begin{center}
\underline{\textbf{Discrete Fourier Transform}}

\textcolor{gray}{\mportant{$\hat{x}[k]=\sum\limits_{n=0}^{N-1}x[n]e^{-2\pi ik\frac{n}{N}}$}}

\mportant{$X(\omega_n)=\sum\limits_{k=0}^{N-1}x(k)e^{-j\omega_n k},\ \omega_n=2\pi\frac{n}{N}$}
\includegraphics[width=\myimagewidth]{Pictures/DFTPeriodicSignal.pdf}
\end{center}
&
\begin{center}
\underline{\textbf{Power Spectral Density (PSD)}}

\vspace{1.5cm}

\mportant{$\phi(\omega_n)=\frac{1}{N}|X(\omega_n)|^2=\sum\limits_{\tau =0}^{N-1}R_x(\tau)e^{-j\omega_n\tau}$}

\includegraphics[width=\myimagewidth]{Pictures/PowerSpectralDensityFromDFT.pdf}
\end{center}
&
\begin{center}
\underline{\textbf{Periodic Autocorrelation}}

\vspace{1.5cm}

\mportant{$R_x(\tau)=\frac{1}{N}\sum\limits_{k=0}^{N-1}x(k)x(k-\tau)$}

\includegraphics[width=\myimagewidth]{Pictures/PeriodicAutocorrelation.pdf}
\end{center}\\\hline
\end{tabular}
\normalsize

\newpage

\subsection{Transfer Function Estimation}

\includegraphics[width=\linewidth]{Pictures/Open-loopETFE.pdf}

\end{document}